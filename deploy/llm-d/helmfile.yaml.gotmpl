# llm-d helmfile — 3-chart deployment pattern
# Charts: llm-d-infra (CRDs), inferencepool (EPP), llm-d-modelservice (vLLM workers)
environments:
  default:
    values:
    - env: default

---
repositories:
# llm-d-deployer (ghcr.io/llm-d) was deprecated Oct 2025.
# Charts now live in llm-d-incubation HTTP repos and upstream GAIE OCI registry.
- name: llm-d-infra
  url: https://llm-d-incubation.github.io/llm-d-infra/
- name: llm-d-modelservice
  url: https://llm-d-incubation.github.io/llm-d-modelservice/
# Gateway API Inference Extension: upstream inferencepool chart
- name: gaie
  url: registry.k8s.io/gateway-api-inference-extension/charts
  oci: true

releases:
  # 1. Infrastructure — InferencePool CRDs and Gateway configuration
  - name: llm-d-infra
    namespace: token-labs
    chart: llm-d-infra/llm-d-infra
    version: v1.3.6
    values:
    - values/infra.yaml

  # 2. InferencePool + EPP — Inference-aware ext_proc routing
  # Chart source: registry.k8s.io/gateway-api-inference-extension/charts/inferencepool
  - name: llm-d-inferencepool
    namespace: token-labs
    chart: gaie/inferencepool
    version: v1.3.0
    needs:
    - token-labs/llm-d-infra
    values:
    - values/inferencepool.yaml

  # 3. Model Service — vLLM Llama workers on DGX Spark GPU nodes
  - name: llm-d-modelservice
    namespace: token-labs
    chart: llm-d-modelservice/llm-d-modelservice
    version: v0.4.5
    needs:
    - token-labs/llm-d-infra
    values:
    - values/modelservice.yaml

  # 4. InferencePool + EPP for Nemotron VL 12B
  - name: llm-d-inferencepool-nemotron-vl
    namespace: token-labs
    chart: gaie/inferencepool
    version: v1.3.0
    needs:
    - token-labs/llm-d-infra
    values:
    - values/inferencepool-nemotron-vl.yaml

  # 5. Model Service — Nemotron VL 12B FP8 on spark-02
  - name: llm-d-modelservice-nemotron-vl
    namespace: token-labs
    chart: llm-d-modelservice/llm-d-modelservice
    version: v0.4.5
    needs:
    - token-labs/llm-d-infra
    values:
    - values/modelservice-nemotron-vl.yaml

  # 6. InferencePool + EPP — Inference-aware ext_proc routing
  # Chart source: registry.k8s.io/gateway-api-inference-extension/charts/inferencepool
  - name: llm-d-inferencepool-qwen3-14b
    namespace: token-labs
    chart: gaie/inferencepool
    version: v1.3.0
    needs:
    - token-labs/llm-d-infra
    values:
    - values/inferencepool-qwen3-14b.yaml

  # 3. Model Service — vLLM Llama workers on DGX Spark GPU nodes
  - name: llm-d-modelservice-qwen3-14b
    namespace: token-labs
    chart: llm-d-modelservice/llm-d-modelservice
    version: v0.4.5
    needs:
    - token-labs/llm-d-infra
    values:
    - values/modelservice-qwen3-14b.yaml
