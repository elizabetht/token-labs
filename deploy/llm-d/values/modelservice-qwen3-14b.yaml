# llm-d-modelservice values — Nemotron-Llama 8B on spark-01
#
# NOTE: spark-01 has ONE GB10 GPU (nvidia.com/gpu.count=1, sharing-strategy=none).
# The spellingbee namespace may also be using this GPU. Scale down spellingbee
# vLLM pods before deploying, or enable MPS time-slicing on spark-01:
#   kubectl label node spark-01 nvidia.com/gpu.sharing-strategy=mps

# Chart v0.4.x renamed: model.name -> modelArtifacts.name, image -> decode.containers[].image
# tensor_parallelism -> decode.parallelism.tensor, extraArgs -> decode.containers[].args

modelArtifacts:
  name: "nvidia/Qwen3-14B-NVFP4"
  # Labels used by the InferencePool matchLabels selector
  labels:
    llm-d.ai/inference-serving: "true"
    llm-d.ai/model: qwen-3-14b

decode:
  replicas: 1
  parallelism:
    tensor: 1
  monitoring:
    podmonitor:
      enabled: true
      portName: "vllm"  # decode vLLM service port (from routing.proxy.targetPort)
      path: "/metrics"
      interval: "30s"

  extraConfig:
    runtimeClassName: nvidia

  # Pin to spark-01 (spark-02 is reserved for Nemotron VL).
  nodeSelector:
    kubernetes.io/hostname: spark-01
    nvidia.com/gpu.product: "NVIDIA-GB10"

  tolerations:
  - key: nvidia.com/gpu
    operator: Exists
    effect: NoSchedule

  containers:
  - name: vllm
    image: "ghcr.io/elizabetht/token-labs/vllm-serve:v0.4.0"
    modelCommand: vllmServe
    resources:
      requests:
        cpu: "4"
        memory: 32Gi
        nvidia.com/gpu: "1"
      limits:
        cpu: "8"
        memory: 64Gi
        nvidia.com/gpu: "1"
    env:
    - name: FLASHINFER_DISABLE_VERSION_CHECK
      value: "1"
    # vLLM exposes GET /health on port 8000 (200 = ready, 503 = loading)
    # startupProbe: up to 10 min for model weights to load before liveness kicks in
    startupProbe:
      httpGet:
        path: /v1/models
        port: 8000
      initialDelaySeconds: 30
      periodSeconds: 10
      failureThreshold: 60   # 30s + 60x10s = up to 10 min
      timeoutSeconds: 10
    readinessProbe:
      httpGet:
        path: /v1/models
        port: 8000
      initialDelaySeconds: 5
      periodSeconds: 15
      failureThreshold: 3
      timeoutSeconds: 5
    livenessProbe:
      httpGet:
        path: /v1/models  
        port: 8000
      periodSeconds: 30
      failureThreshold: 5
      timeoutSeconds: 10
    # vLLM engine arguments for Nemotron-Llama 8B (BF16, fits in 72GB GB10 GPU)
    args:
    - "--dtype=bfloat16"
    - "--gpu-memory-utilization=0.4"

# HuggingFace token — required if the model is gated on HuggingFace.
# Create secret: kubectl create secret generic hf-token-secret \
#   --from-literal=hf-token=hf_xxx -n token-labs
# modelArtifacts:
#   authSecretName: hf-token-secret

prefill:
  replicas: 0
