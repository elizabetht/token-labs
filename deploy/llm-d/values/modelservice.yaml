# llm-d-modelservice values — vLLM workers on DGX Spark GPU nodes
model:
  name: "meta-llama/Llama-3.1-8B-Instruct"

image:
  repository: ghcr.io/llm-d/llm-d-cuda
  tag: v0.5.0

# Decode workers — one on spark-01 (spark-02 runs Nemotron VL)
decode:
  replicas: 1
  tensor_parallelism: 1

  resources:
    requests:
      cpu: "4"
      memory: 32Gi
      nvidia.com/gpu: "1"
    limits:
      cpu: "8"
      memory: 64Gi
      nvidia.com/gpu: "1"

  # Node affinity — schedule on DGX Spark GPU workers
  nodeSelector:
    nvidia.com/gpu.product: "GH200"

  tolerations:
  - key: nvidia.com/gpu
    operator: Exists
    effect: NoSchedule

  # vLLM engine arguments
  extraArgs:
  - "--max-model-len=8192"
  - "--dtype=auto"
  - "--enforce-eager"
  - "--gpu-memory-utilization=0.90"

# HuggingFace token for gated models (if needed)
# huggingFaceSecret: hf-token-secret

# Prefill workers (disabled for single-tier setup)
prefill:
  replicas: 0
