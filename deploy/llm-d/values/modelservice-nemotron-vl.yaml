# llm-d-modelservice values — Nemotron VL 12B FP8 on spark-02
#
# NOTE: spark-02 has ONE GB10 GPU (72GB HBM3e). The spellingbee namespace may
# also be using this GPU (vllm-nemotron-nano-vl-8b). Scale it down first:
#   kubectl scale deployment vllm-nemotron-nano-vl-8b -n spellingbee --replicas=0

modelArtifacts:
  name: "nvidia/NVIDIA-Nemotron-Nano-12B-v2-VL-FP8"
  # Labels used by the InferencePool matchLabels selector
  labels:
    llm-d.ai/inference-serving: "true"
    llm-d.ai/model: nemotron-vl-12b

decode:
  replicas: 1
  parallelism:
    tensor: 1

  # runtimeClassName must go through extraConfig — the chart template has no
  # direct runtimeClassName block; only nodeSelector/tolerations are explicit.
  extraConfig:
    runtimeClassName: nvidia

  # Pin to spark-02. spark-01 is reserved for Nemotron-Llama.
  nodeSelector:
    kubernetes.io/hostname: spark-02
    nvidia.com/gpu.product: "NVIDIA-GB10"

  tolerations:
  - key: nvidia.com/gpu
    operator: Exists
    effect: NoSchedule

  containers:
  - name: vllm
    image: "ghcr.io/elizabetht/token-labs/vllm-serve:v0.4.0"
    modelCommand: vllmServe
    resources:
      requests:
        cpu: "4"
        memory: 32Gi
        nvidia.com/gpu: "1"
      limits:
        cpu: "8"
        memory: 64Gi
        nvidia.com/gpu: "1"
    # vLLM arguments for Nemotron VL 12B FP8
    # --quantization=modelopt: uses NVIDIA ModelOpt FP8 calibration data
    # --trust-remote-code: required for Nemotron VL custom model code
    env:
    - name: FLASHINFER_DISABLE_VERSION_CHECK
      value: "1"
    args:
    - "--trust-remote-code"
    - "--quantization=modelopt"
    - "--dtype=auto"
    - "--enforce-eager"
    - "--gpu-memory-utilization=0.90"

# modelArtifacts:
#   authSecretName: hf-token-secret

prefill:
  replicas: 0
