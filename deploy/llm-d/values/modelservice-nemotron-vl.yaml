# llm-d-modelservice values — Nemotron VL 12B FP8 on spark-02
#
# NOTE: spark-02 has ONE GB10 GPU (72GB HBM3e). The spellingbee namespace may
# also be using this GPU (vllm-nemotron-nano-vl-8b). Scale it down first:
#   kubectl scale deployment vllm-nemotron-nano-vl-8b -n spellingbee --replicas=0

modelArtifacts:
  name: "nvidia/NVIDIA-Nemotron-Nano-12B-v2-VL-FP8"
  # Labels used by the InferencePool matchLabels selector
  labels:
    llm-d.ai/inference-serving: "true"
    llm-d.ai/model: nemotron-vl-12b

decode:
  replicas: 1
  parallelism:
    tensor: 1
  monitoring:
    podmonitor:
      enabled: true
      portName: "vllm"  # decode vLLM service port (from routing.proxy.targetPort)
      path: "/metrics"
      interval: "30s"

  # runtimeClassName must go through extraConfig — the chart template has no
  # direct runtimeClassName block; only nodeSelector/tolerations are explicit.
  extraConfig:
    runtimeClassName: nvidia

  # Pin to spark-02. spark-01 is reserved for Nemotron-Llama.
  nodeSelector:
    kubernetes.io/hostname: spark-02
    nvidia.com/gpu.product: "NVIDIA-GB10-SHARED"

  tolerations:
  - key: nvidia.com/gpu
    operator: Exists
    effect: NoSchedule

  containers:
  - name: vllm
    image: "ghcr.io/elizabetht/token-labs/vllm-serve:v0.4.0"
    modelCommand: vllmServe
    resources:
      requests:
        cpu: "4"
        memory: 32Gi
        nvidia.com/gpu: "1"
      limits:
        cpu: "8"
        memory: 64Gi
        nvidia.com/gpu: "1"
    env:
    - name: FLASHINFER_DISABLE_VERSION_CHECK
      value: "1"
    # vLLM exposes GET /health on port 8000 (200 = ready, 503 = loading)
    # startupProbe: up to 15 min for FP8 model + trust-remote-code compilation
    startupProbe:
      httpGet:
        path: /v1/models
        port: 8000
      initialDelaySeconds: 30
      periodSeconds: 10
      failureThreshold: 90   # 30s + 90x10s = up to 15 min
      timeoutSeconds: 5
    readinessProbe:
      httpGet:
        path: /v1/models
        port: 8000
      periodSeconds: 15
      failureThreshold: 3
      timeoutSeconds: 5
    livenessProbe:
      httpGet:
        path: /health
        port: 8000
      periodSeconds: 30
      failureThreshold: 3
      timeoutSeconds: 10
    args:
    - "--trust-remote-code"
    - "--quantization=modelopt"
    - "--dtype=auto"
    - "--enforce-eager"
    - "--gpu-memory-utilization=0.90"

# modelArtifacts:
#   authSecretName: hf-token-secret

prefill:
  replicas: 0
