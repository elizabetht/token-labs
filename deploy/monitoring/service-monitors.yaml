# ServiceMonitor for Prometheus-based observability
#
# Monitors:
# - Limitador metrics (rate limit counters, token usage)
# - Authorino metrics (auth decisions)
# - Envoy Gateway proxy metrics
#
# Requires: prometheus-operator or kube-prometheus-stack
---
# Limitador metrics — token consumption and rate limit counters
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: limitador-metrics
  namespace: kuadrant-system
  labels:
    app.kubernetes.io/part-of: token-labs
spec:
  selector:
    matchLabels:
      app: limitador
  endpoints:
  - port: metrics
    interval: 15s
---
# Authorino metrics — auth decisions per tenant
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: authorino-metrics
  namespace: kuadrant-system
  labels:
    app.kubernetes.io/part-of: token-labs
spec:
  selector:
    matchLabels:
      app: authorino
  endpoints:
  - port: metrics
    interval: 15s
---
# Envoy Gateway proxy metrics
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: envoy-proxy-metrics
  namespace: token-labs
  labels:
    app.kubernetes.io/part-of: token-labs
spec:
  selector:
    matchLabels:
      gateway.envoyproxy.io/owning-gateway-name: token-labs-gateway
  endpoints:
  - port: metrics
    interval: 15s
---
# vLLM worker metrics — GPU cache usage, queue depth, and throughput
#
# Key metrics for GPU-aware HPA:
#   vllm:num_requests_waiting      — queue depth (primary HPA signal)
#   vllm:gpu_cache_usage_perc      — VRAM KV-cache saturation (0.0–1.0)
#   vllm:avg_generation_throughput_toks_per_s — real-world throughput health
#
# vLLM exposes Prometheus metrics at GET /metrics on the same port as the API (8000).
# The llm-d-modelservice chart creates a headless Service per model with a "http" port.
# This ServiceMonitor scrapes all Services labeled llm-d.ai/inference-serving=true.
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: vllm-worker-metrics
  namespace: token-labs
  labels:
    app.kubernetes.io/part-of: token-labs
    app.kubernetes.io/component: autoscaling
spec:
  selector:
    matchLabels:
      llm-d.ai/inference-serving: "true"
  endpoints:
  - port: http
    path: /metrics
    interval: 15s
