# HTTPRoute — Routes OpenAI-compatible API paths to the llm-d InferencePool
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: llm-inference
  namespace: token-labs
spec:
  parentRefs:
  - name: token-labs-gateway
    namespace: token-labs
  hostnames:
  - "inference.token-labs.local"
  rules:
  # Chat completions endpoint
  - matches:
    - path:
        type: PathPrefix
        value: /v1/chat/completions
      method: POST
    backendRefs:
    - group: inference.networking.x-k8s.io
      kind: InferencePool
      name: token-labs-pool
  # Models listing endpoint
  - matches:
    - path:
        type: Exact
        value: /v1/models
      method: GET
    backendRefs:
    - group: inference.networking.x-k8s.io
      kind: InferencePool
      name: token-labs-pool
  # Completions endpoint
  - matches:
    - path:
        type: PathPrefix
        value: /v1/completions
      method: POST
    backendRefs:
    - group: inference.networking.x-k8s.io
      kind: InferencePool
      name: token-labs-pool
  # Health check (no auth needed — handled by separate route if desired)
  - matches:
    - path:
        type: Exact
        value: /health
      method: GET
    backendRefs:
    - group: inference.networking.x-k8s.io
      kind: InferencePool
      name: token-labs-pool
