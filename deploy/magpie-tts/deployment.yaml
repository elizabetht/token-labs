# Magpie TTS — NeMo-based text-to-speech service (CPU mode)
#
# GPU CONFLICT NOTE:
# spark-01 has one GB10 GPU with sharing-strategy=none. That GPU is fully
# allocated to the Nemotron-Llama vLLM pod. Magpie TTS runs on CPU here.
# CPU inference for the 357M TTS model is acceptable for moderate traffic
# (typical TTS latency: ~500ms–2s on CPU for short utterances).
#
# To run Magpie TTS on GPU: enable MPS time-slicing on spark-01:
#   kubectl patch clusterpolicy gpu-cluster-policy -n gpu-operator --type=merge \
#     -p '{"spec":{"devicePlugin":{"config":{"name":"time-slicing-config"}}}}'
# Then add nvidia.com/gpu: "1" back to resources and uncomment tolerations.
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: magpie-tts
  namespace: token-labs
  labels:
    app: magpie-tts
    app.kubernetes.io/part-of: token-labs
spec:
  replicas: 1
  selector:
    matchLabels:
      app: magpie-tts
  template:
    metadata:
      labels:
        app: magpie-tts
    spec:
      nodeSelector:
        kubernetes.io/hostname: spark-01
      containers:
      - name: magpie-tts
        image: ghcr.io/elizabetht/token-labs/magpie-tts:latest
        ports:
        - containerPort: 8000
          name: http
          protocol: TCP
        resources:
          requests:
            cpu: "2"
            memory: 4Gi
          limits:
            cpu: "4"
            memory: 8Gi
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 30
---
apiVersion: v1
kind: Service
metadata:
  name: magpie-tts
  namespace: token-labs
  labels:
    app: magpie-tts
    app.kubernetes.io/part-of: token-labs
spec:
  selector:
    app: magpie-tts
  ports:
  - port: 8000
    targetPort: 8000
    name: http
    protocol: TCP
