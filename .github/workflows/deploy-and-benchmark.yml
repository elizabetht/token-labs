name: Deploy and Benchmark on DGX Spark

on:
  workflow_run:
    workflows: ["Build and push vLLM image"]
    types: [completed]
  workflow_dispatch:
    inputs:
      image_tag:
        description: 'Image tag to deploy (default: latest)'
        required: false
        default: 'latest'

permissions:
  contents: write
  packages: read

env:
  CONTAINER_NAME: vllm-server
  VLLM_PORT: 8000
  MODEL: meta-llama/Llama-3.1-8B-Instruct
  # DGX Spark economics: $4000 hardware / 26280 hours (3yr) with 0.3 utilization = $0.05/hr + ~$0.02/hr electricity = $0.07/hr
  DGX_COST_PER_HOUR: "0.07"
  DGX_TAILSCALE_IP: "100.64.38.13"

jobs:
  deploy-and-benchmark:
    runs-on: [self-hosted, DGX-Spark]
    # Only run if build succeeded or manually triggered
    if: ${{ github.event.workflow_run.conclusion == 'success' || github.event_name == 'workflow_dispatch' }}
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Fix Git ownership
        run: git config --global --add safe.directory "$GITHUB_WORKSPACE"

      - name: Determine image tag
        id: vars
        run: |
          OWNER=$(echo "${GITHUB_REPOSITORY_OWNER}" | tr '[:upper:]' '[:lower:]')
          IMAGE="ghcr.io/${OWNER}/token-labs/vllm-serve"
          
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            TAG="${{ github.event.inputs.image_tag }}"
          else
            # Get tag from the workflow that triggered this
            TAG="latest"
          fi
          
          echo "IMAGE=${IMAGE}" >> "$GITHUB_OUTPUT"
          echo "TAG=${TAG}" >> "$GITHUB_OUTPUT"
          echo "FULL_IMAGE=${IMAGE}:${TAG}" >> "$GITHUB_OUTPUT"

      - name: Log in to GHCR
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Pull latest image
        run: |
          echo "Pulling image: ${{ steps.vars.outputs.FULL_IMAGE }}"
          docker pull ${{ steps.vars.outputs.FULL_IMAGE }}

      - name: Stop existing vLLM container (if running)
        run: |
          if docker ps -q -f name=${{ env.CONTAINER_NAME }} | grep -q .; then
            echo "Stopping existing container..."
            docker stop ${{ env.CONTAINER_NAME }}
          fi
          if docker ps -aq -f name=${{ env.CONTAINER_NAME }} | grep -q .; then
            echo "Removing existing container..."
            docker rm ${{ env.CONTAINER_NAME }}
          fi

      - name: Start vLLM server
        run: |
          echo "Starting vLLM server with model: ${{ env.MODEL }}"
          docker run -d \
            --gpus all \
            --name ${{ env.CONTAINER_NAME }} \
            -p ${{ env.DGX_TAILSCALE_IP}}:${{ env.VLLM_PORT }}:8000 \
            -e HF_TOKEN=${{ secrets.HF_TOKEN }} \
            -e FLASHINFER_DISABLE_VERSION_CHECK=1 \
            -v $HOME/.cache/huggingface:/root/.cache/huggingface \
            ${{ steps.vars.outputs.FULL_IMAGE }} \
            ${{ env.MODEL }} \
            --gpu-memory-utilization 0.3 \
            --max-model-len 131072

      - name: Wait for vLLM server to be ready
        run: |
          echo "Waiting for vLLM server to be ready..."
          MAX_ATTEMPTS=120
          ATTEMPT=0
          
          while [ $ATTEMPT -lt $MAX_ATTEMPTS ]; do
            if curl -s http://${{ env.DGX_TAILSCALE_IP}}:${{ env.VLLM_PORT }}/health > /dev/null 2>&1; then
              echo "âœ… vLLM server is ready!"
              exit 0
            fi
            
            ATTEMPT=$((ATTEMPT + 1))
            echo "Waiting for server... ($ATTEMPT/$MAX_ATTEMPTS)"
            
            # Check if container is still running
            if ! docker ps -q -f name=${{ env.CONTAINER_NAME }} | grep -q .; then
              echo "âŒ Container stopped unexpectedly!"
              docker logs ${{ env.CONTAINER_NAME }} --tail 100
              exit 1
            fi
            
            sleep 5
          done
          
          echo "âŒ Server failed to start within timeout"
          docker logs ${{ env.CONTAINER_NAME }} --tail 100
          exit 1

      - name: Run vLLM serving benchmark
        id: benchmark
        run: |
          # Run separate benchmarks for prefill-heavy and decode-heavy workloads
          echo "Running vLLM serving benchmarks..."
          DGX_COST=${{ env.DGX_COST_PER_HOUR }}
          
          # === PREFILL-HEAVY BENCHMARK ===
          # 3:1 input:output ratio to measure prefill (input) performance
          # 3072 input + 1024 output = 4096 total tokens per request
          echo "=== PREFILL-HEAVY BENCHMARK (3:1 input:output ratio) ==="
          docker exec ${{ env.CONTAINER_NAME }} \
            bash -c "source /opt/venv/bin/activate && \
            FLASHINFER_DISABLE_VERSION_CHECK=1 \
            vllm bench serve \
            --model ${{ env.MODEL }} \
            --base-url http://${{ env.DGX_TAILSCALE_IP }}:${{ env.VLLM_PORT }} \
            --num-prompts 100 \
            --request-rate 10 \
            --random-input-len 3072 \
            --random-output-len 1024" \
            2>&1 | tee prefill_bench.txt
          
          cat prefill_bench.txt
          
          # Extract prefill throughput (input tokens/s)
          PREFILL_TPS=$(grep -oP 'Input throughput \(tok/s\):\s*\K[\d.]+' prefill_bench.txt | tail -1 || echo "0")
          if [ "$PREFILL_TPS" = "0" ] || [ -z "$PREFILL_TPS" ]; then
            # Fallback to total throughput if input not found
            PREFILL_TPS=$(grep -oP 'Total Token throughput \(tok/s\):\s*\K[\d.]+' prefill_bench.txt | tail -1 || echo "0")
          fi
          echo "Prefill throughput: $PREFILL_TPS tokens/s"
          
          # === DECODE-HEAVY BENCHMARK ===
          # 1:3 input:output ratio to measure decode (output) performance
          # 1024 input + 3072 output = 4096 total tokens per request
          echo ""
          echo "=== DECODE-HEAVY BENCHMARK (1:3 input:output ratio) ==="
          docker exec ${{ env.CONTAINER_NAME }} \
            bash -c "source /opt/venv/bin/activate && \
            FLASHINFER_DISABLE_VERSION_CHECK=1 \
            vllm bench serve \
            --model ${{ env.MODEL }} \
            --base-url http://${{ env.DGX_TAILSCALE_IP }}:${{ env.VLLM_PORT }} \
            --num-prompts 100 \
            --request-rate 10 \
            --random-input-len 1024 \
            --random-output-len 3072" \
            2>&1 | tee decode_bench.txt
          
          cat decode_bench.txt
          
          # Extract decode throughput (output tokens/s)
          DECODE_TPS=$(grep -oP 'Output throughput \(tok/s\):\s*\K[\d.]+' decode_bench.txt | tail -1 || echo "0")
          if [ "$DECODE_TPS" = "0" ] || [ -z "$DECODE_TPS" ]; then
            # Fallback to total throughput if output not found
            DECODE_TPS=$(grep -oP 'Total Token throughput \(tok/s\):\s*\K[\d.]+' decode_bench.txt | tail -1 || echo "0")
          fi
          echo "Decode throughput: $DECODE_TPS tokens/s"
          
          # Use extracted values
          INPUT_TPS=$PREFILL_TPS
          OUTPUT_TPS=$DECODE_TPS
          
          echo ""
          echo "=== FINAL RESULTS ==="
          echo "Input (prefill) throughput: $INPUT_TPS tokens/s"
          echo "Output (decode) throughput: $OUTPUT_TPS tokens/s"
          
          # Calculate cost per 1M tokens: (cost_per_hour / tokens_per_sec) * 1_000_000 / 3600
          
          if [ "$INPUT_TPS" != "0" ] && [ -n "$INPUT_TPS" ]; then
            # Use awk to ensure proper decimal formatting (bc outputs .xxx instead of 0.xxx)
            COST_IN=$(echo "scale=4; $DGX_COST * 1000000 / ($INPUT_TPS * 3600)" | bc | awk '{printf "%.4f", $0}')
          else
            COST_IN="0"
          fi
          
          if [ "$OUTPUT_TPS" != "0" ] && [ -n "$OUTPUT_TPS" ]; then
            COST_OUT=$(echo "scale=4; $DGX_COST * 1000000 / ($OUTPUT_TPS * 3600)" | bc | awk '{printf "%.4f", $0}')
          else
            COST_OUT="0"
          fi
          
          echo "Cost per 1M input tokens: \$$COST_IN"
          echo "Cost per 1M output tokens: \$$COST_OUT"
          
          # Ensure variables have valid numeric values for JSON
          # Default to 0 if empty or unset
          INPUT_TPS="${INPUT_TPS:-0}"
          OUTPUT_TPS="${OUTPUT_TPS:-0}"
          COST_IN="${COST_IN:-0}"
          COST_OUT="${COST_OUT:-0}"
          
          # Handle empty strings explicitly
          [[ -z "$INPUT_TPS" || "$INPUT_TPS" == "" ]] && INPUT_TPS="0"
          [[ -z "$OUTPUT_TPS" || "$OUTPUT_TPS" == "" ]] && OUTPUT_TPS="0"
          [[ -z "$COST_IN" || "$COST_IN" == "" ]] && COST_IN="0"
          [[ -z "$COST_OUT" || "$COST_OUT" == "" ]] && COST_OUT="0"
          
          # Debug: print values before JSON generation
          echo "DEBUG: INPUT_TPS='$INPUT_TPS' OUTPUT_TPS='$OUTPUT_TPS' COST_IN='$COST_IN' COST_OUT='$COST_OUT'"
          
          # Save results to JSON using Python for proper JSON formatting
          python3 -c "
          import json
          from datetime import datetime
          
          data = {
              'model': '${{ env.MODEL }}',
              'dgx_cost_per_hour': float('${DGX_COST}' or '0'),
              'image_tag': '${{ steps.vars.outputs.FULL_IMAGE }}',
              'prefill': {
                  'tokens_per_second': float('${INPUT_TPS}' or '0'),
                  'cost_per_million_tokens': float('${COST_IN}' or '0')
              },
              'decode': {
                  'tokens_per_second': float('${OUTPUT_TPS}' or '0'),
                  'cost_per_million_tokens': float('${COST_OUT}' or '0')
              },
              'timestamp': datetime.utcnow().isoformat() + 'Z',
              'vllm_server_args': {
                  'gpu_memory_utilization': 0.3,
                  'max_model_len': 131072
              },
              'benchmark_args': {
                  'prefill_test': {
                      'num_prompts': 100,
                      'request_rate': 10,
                      'input_len': 3072,
                      'output_len': 1024,
                      'ratio': '3:1 input:output',
                      'total_tokens': 4096
                  },
                  'decode_test': {
                      'num_prompts': 100,
                      'request_rate': 10,
                      'input_len': 1024,
                      'output_len': 3072,
                      'ratio': '1:3 input:output',
                      'total_tokens': 4096
                  }
              },
              'hardware': {
                  'platform': 'NVIDIA DGX Spark',
                  'gpu': 'Grace Hopper',
                  'architecture': 'ARM64'
              }
          }
          
          with open('bench_results.json', 'w') as f:
              json.dump(data, f, indent=2)
          
          print(json.dumps(data, indent=2))
          "
          
          # Set outputs for summary
          echo "prefill_tps=$INPUT_TPS" >> $GITHUB_OUTPUT
          echo "decode_tps=$OUTPUT_TPS" >> $GITHUB_OUTPUT
          echo "cost_in=$COST_IN" >> $GITHUB_OUTPUT
          echo "cost_out=$COST_OUT" >> $GITHUB_OUTPUT

      - name: Update pricing in docs
        if: success()
        run: |
          python3 scripts/update_pricing.py \
            --results bench_results.json \
            --html docs/index.html
          
          # Also copy results to docs for GitHub Pages
          cp bench_results.json docs/benchmark-results.json
          
          # Append to benchmark history
          python3 -c "
          import json
          from datetime import datetime
          
          # Load new results
          with open('bench_results.json', 'r') as f:
              new_result = json.load(f)
          
          # Add version from image tag
          new_result['version'] = '${{ steps.vars.outputs.TAG }}'
          
          # Load existing history
          history_file = 'docs/benchmark-history.json'
          try:
              with open(history_file, 'r') as f:
                  history = json.load(f)
          except (FileNotFoundError, json.JSONDecodeError):
              history = []
          
          # Check if this version already exists (update it) or add new
          version_exists = False
          for i, entry in enumerate(history):
              if entry.get('version') == new_result['version']:
                  history[i] = new_result
                  version_exists = True
                  break
          
          if not version_exists:
              history.append(new_result)
          
          # Keep only last 50 entries
          history = history[-50:]
          
          # Save updated history
          with open(history_file, 'w') as f:
              json.dump(history, f, indent=2)
          
          print(f'History updated: {len(history)} entries')
          "

      - name: Commit and push pricing updates
        if: success()
        run: |
          cd $GITHUB_WORKSPACE
          pwd
          ls -la docs/
          git status
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          # Check if there are any changes to commit
          if git diff --quiet docs/index.html docs/benchmark-results.json docs/benchmark-history.json 2>/dev/null; then
            echo "No pricing changes to commit"
          else
            git add docs/index.html docs/benchmark-results.json docs/benchmark-history.json
            git commit -m "Update pricing and benchmark results [skip ci]"
            git push
            echo "âœ… Pricing and benchmark results updated and pushed"
          fi

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: bench_results.json

      - name: Add summary
        run: |
          echo "## ðŸš€ Deployment and Benchmark Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Image:** \`${{ steps.vars.outputs.FULL_IMAGE }}\`" >> $GITHUB_STEP_SUMMARY
          echo "- **Model:** \`${{ env.MODEL }}\`" >> $GITHUB_STEP_SUMMARY
          echo "- **Container:** \`${{ env.CONTAINER_NAME }}\` running on port ${{ env.VLLM_PORT }}" >> $GITHUB_STEP_SUMMARY
          echo "- **DGX Spark Cost:** \`\$${{ env.DGX_COST_PER_HOUR }}/hour\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Benchmark Results (vllm bench latency)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Prefill (Input) | Decode (Output) |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-----------------|-----------------|" >> $GITHUB_STEP_SUMMARY
          echo "| Throughput | ${{ steps.benchmark.outputs.prefill_tps }} tok/s | ${{ steps.benchmark.outputs.decode_tps }} tok/s |" >> $GITHUB_STEP_SUMMARY
          echo "| Cost per 1M tokens | \$${{ steps.benchmark.outputs.cost_in }} | \$${{ steps.benchmark.outputs.cost_out }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f bench_results.json ]; then
            echo "### Raw Results" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`json" >> $GITHUB_STEP_SUMMARY
            cat bench_results.json >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          fi
