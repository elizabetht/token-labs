name: Deploy and Benchmark on DGX Spark

on:
  workflow_run:
    workflows: ["Build and push vLLM image"]
    types: [completed]
  workflow_dispatch:
    inputs:
      image_tag:
        description: 'Image tag to deploy (default: latest)'
        required: false
        default: 'latest'

permissions:
  contents: write
  packages: read

env:
  CONTAINER_NAME: vllm-server
  VLLM_PORT: 8000
  MODEL: meta-llama/Llama-3.1-8B-Instruct
  # DGX Spark economics: $4000 hardware / 26280 hours (3yr) with 0.3 utilization = $0.05/hr + ~$0.02/hr electricity = $0.07/hr
  DGX_COST_PER_HOUR: "0.07"
  DGX_TAILSCALE_IP: "100.64.38.13"
  # LMCache settings
  LMCACHE_ENABLED: "true"

jobs:
  deploy-and-benchmark:
    runs-on: [self-hosted, DGX-Spark]
    # Only run if build succeeded or manually triggered
    if: ${{ github.event.workflow_run.conclusion == 'success' || github.event_name == 'workflow_dispatch' }}
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Fix Git ownership
        run: git config --global --add safe.directory "$GITHUB_WORKSPACE"

      - name: Determine image tag
        id: vars
        run: |
          OWNER=$(echo "${GITHUB_REPOSITORY_OWNER}" | tr '[:upper:]' '[:lower:]')
          IMAGE="ghcr.io/${OWNER}/token-labs/vllm-serve"
          
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            TAG="${{ github.event.inputs.image_tag }}"
          else
            # Get version from the tag that triggered the build workflow
            # workflow_run event provides head_branch which contains the tag name
            TAG="${{ github.event.workflow_run.head_branch }}"
            # Remove 'v' prefix if present (v0.1.0 -> 0.1.0)
            TAG="${TAG#v}"
            # Fallback to latest if tag is empty
            if [ -z "$TAG" ]; then
              TAG="latest"
            fi
          fi
          
          echo "IMAGE=${IMAGE}" >> "$GITHUB_OUTPUT"
          echo "TAG=${TAG}" >> "$GITHUB_OUTPUT"
          echo "FULL_IMAGE=${IMAGE}:${TAG}" >> "$GITHUB_OUTPUT"
          echo "Using image tag: $TAG"

      - name: Log in to GHCR
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Pull latest image
        run: |
          echo "Pulling image: ${{ steps.vars.outputs.FULL_IMAGE }}"
          docker pull ${{ steps.vars.outputs.FULL_IMAGE }}

      - name: Stop existing vLLM container (if running)
        run: |
          if docker ps -q -f name=${{ env.CONTAINER_NAME }} | grep -q .; then
            echo "Stopping existing container..."
            docker stop ${{ env.CONTAINER_NAME }}
          fi
          if docker ps -aq -f name=${{ env.CONTAINER_NAME }} | grep -q .; then
            echo "Removing existing container..."
            docker rm ${{ env.CONTAINER_NAME }}
          fi

      - name: Start vLLM server
        run: |
          echo "Starting vLLM server with model: ${{ env.MODEL }} (LMCache enabled)"
          docker run -d \
            --gpus all \
            --name ${{ env.CONTAINER_NAME }} \
            -p ${{ env.DGX_TAILSCALE_IP}}:${{ env.VLLM_PORT }}:8000 \
            -e HF_TOKEN=${{ secrets.HF_TOKEN }} \
            -e FLASHINFER_DISABLE_VERSION_CHECK=1 \
            -e LMCACHE_CONFIG_FILE=/app/config/lmcache-cpu-offload.yaml \
            -e LMCACHE_USE_EXPERIMENTAL=True \
            -v $HOME/.cache/huggingface:/root/.cache/huggingface \
            -v ${{ github.workspace }}/config:/app/config:ro \
            ${{ steps.vars.outputs.FULL_IMAGE }} \
            ${{ env.MODEL }} \
            --gpu-memory-utilization 0.3

      - name: Wait for vLLM server to be ready
        run: |
          echo "Waiting for vLLM server to be ready..."
          MAX_ATTEMPTS=300
          ATTEMPT=0
          while [ $ATTEMPT -lt $MAX_ATTEMPTS ]; do
            if curl -s http://${{ env.DGX_TAILSCALE_IP}}:${{ env.VLLM_PORT }}/health > /dev/null 2>&1; then
              echo "âœ… vLLM server is ready!"
              break
            fi
            ATTEMPT=$((ATTEMPT+1))
            sleep 1
          done
          if [ $ATTEMPT -eq $MAX_ATTEMPTS ]; then
            echo "âŒ Server failed to start within timeout"
            docker logs ${{ env.CONTAINER_NAME }} --tail 100
            exit 1
          fi

      - name: Run vLLM serving benchmark
        id: benchmark
        run: |
          # Run separate benchmarks for prefill-heavy and decode-heavy workloads
          echo "Running vLLM serving benchmarks..."
          DGX_COST=${{ env.DGX_COST_PER_HOUR }}
          
          # === PREFILL-HEAVY BENCHMARK ===
          # 8:1 input:output ratio to measure prefill (input) performance
          # 8192 input + 1024 output = 9216 total tokens per request
          echo "=== PREFILL-HEAVY BENCHMARK (8:1 input:output ratio) ==="
          docker exec ${{ env.CONTAINER_NAME }} \
            bash -c "source /opt/venv/bin/activate && \
            FLASHINFER_DISABLE_VERSION_CHECK=1 \
            vllm bench serve \
            --model ${{ env.MODEL }} \
            --base-url http://${{ env.DGX_TAILSCALE_IP }}:${{ env.VLLM_PORT }} \
            --num-prompts 100 \
            --request-rate 10 \
            --random-input-len 8192 \
            --random-output-len 1024" \
            2>&1 | tee prefill_bench.txt
          
          cat prefill_bench.txt
          
          # Extract prefill throughput (input tokens/s)
          PREFILL_TPS=$(grep -oP 'Input throughput \(tok/s\):\s*\K[\d.]+' prefill_bench.txt | tail -1 || echo "0")
          if [ "$PREFILL_TPS" = "0" ] || [ -z "$PREFILL_TPS" ]; then
            # Fallback to total throughput if input not found
            PREFILL_TPS=$(grep -oP 'Total Token throughput \(tok/s\):\s*\K[\d.]+' prefill_bench.txt | tail -1 || echo "0")
          fi
          echo "Prefill throughput: $PREFILL_TPS tokens/s"
          
          # Extract cached input tokens throughput (for LMCache)
          CACHED_TPS=$(grep -oP 'Cached input throughput \(tok/s\):\s*\K[\d.]+' prefill_bench.txt | tail -1 || echo "0")
          echo "Cached input throughput: $CACHED_TPS tokens/s"
          
          # === DECODE-HEAVY BENCHMARK ===
          # 1:8 input:output ratio to measure decode (output) performance
          # 1024 input + 8192 output = 9216 total tokens per request
          echo ""
          echo "=== DECODE-HEAVY BENCHMARK (1:8 input:output ratio) ==="
          docker exec ${{ env.CONTAINER_NAME }} \
            bash -c "source /opt/venv/bin/activate && \
            FLASHINFER_DISABLE_VERSION_CHECK=1 \
            vllm bench serve \
            --model ${{ env.MODEL }} \
            --base-url http://${{ env.DGX_TAILSCALE_IP }}:${{ env.VLLM_PORT }} \
            --num-prompts 100 \
            --request-rate 10 \
            --random-input-len 1024 \
            --random-output-len 8192" \
            2>&1 | tee decode_bench.txt
          
          cat decode_bench.txt
          
          # Extract decode throughput (output tokens/s)
          DECODE_TPS=$(grep -oP 'Output throughput \(tok/s\):\s*\K[\d.]+' decode_bench.txt | tail -1 || echo "0")
          if [ "$DECODE_TPS" = "0" ] || [ -z "$DECODE_TPS" ]; then
            # Fallback to total throughput if output not found
            DECODE_TPS=$(grep -oP 'Total Token throughput \(tok/s\):\s*\K[\d.]+' decode_bench.txt | tail -1 || echo "0")
          fi
          echo "Decode throughput: $DECODE_TPS tokens/s"
          
          # Use extracted values
          INPUT_TPS=$PREFILL_TPS
          OUTPUT_TPS=$DECODE_TPS
          
          echo ""
          echo "=== FINAL RESULTS ==="
          echo "Input (prefill) throughput: $INPUT_TPS tokens/s"
          echo "Cached input throughput: $CACHED_TPS tokens/s"
          echo "Output (decode) throughput: $OUTPUT_TPS tokens/s"
          
          # Calculate cost per 1M tokens: (cost_per_hour / tokens_per_sec) * 1_000_000 / 3600
          
          if [ "$INPUT_TPS" != "0" ] && [ -n "$INPUT_TPS" ]; then
            # Use awk to ensure proper decimal formatting (bc outputs .xxx instead of 0.xxx)
            COST_IN=$(echo "scale=4; $DGX_COST * 1000000 / ($INPUT_TPS * 3600)" | bc | awk '{printf "%.4f", $0}')
          else
            COST_IN="0"
          fi
          
          if [ "$OUTPUT_TPS" != "0" ] && [ -n "$OUTPUT_TPS" ]; then
            COST_OUT=$(echo "scale=4; $DGX_COST * 1000000 / ($OUTPUT_TPS * 3600)" | bc | awk '{printf "%.4f", $0}')
          else
            COST_OUT="0"
          fi
          
          # Calculate cost for cached tokens (should be cheaper)
          if [ "$CACHED_TPS" != "0" ] && [ -n "$CACHED_TPS" ]; then
            COST_CACHED=$(echo "scale=4; $DGX_COST * 1000000 / ($CACHED_TPS * 3600)" | bc | awk '{printf "%.4f", $0}')
          else
            COST_CACHED="0"
          fi
          
          echo "Cost per 1M input tokens: \$$COST_IN"
          echo "Cost per 1M cached tokens: \$$COST_CACHED"
          echo "Cost per 1M output tokens: \$$COST_OUT"
          
          # Ensure variables have valid numeric values for JSON
          # Default to 0 if empty or unset
          INPUT_TPS="${INPUT_TPS:-0}"
          OUTPUT_TPS="${OUTPUT_TPS:-0}"
          CACHED_TPS="${CACHED_TPS:-0}"
          COST_IN="${COST_IN:-0}"
          COST_OUT="${COST_OUT:-0}"
          COST_CACHED="${COST_CACHED:-0}"
          
          # Handle empty strings explicitly
          [[ -z "$INPUT_TPS" || "$INPUT_TPS" == "" ]] && INPUT_TPS="0"
          [[ -z "$OUTPUT_TPS" || "$OUTPUT_TPS" == "" ]] && OUTPUT_TPS="0"
          [[ -z "$CACHED_TPS" || "$CACHED_TPS" == "" ]] && CACHED_TPS="0"
          [[ -z "$COST_IN" || "$COST_IN" == "" ]] && COST_IN="0"
          [[ -z "$COST_OUT" || "$COST_OUT" == "" ]] && COST_OUT="0"
          [[ -z "$COST_CACHED" || "$COST_CACHED" == "" ]] && COST_CACHED="0"
          
          # Debug: print values before JSON generation
          echo "DEBUG: INPUT_TPS='$INPUT_TPS' OUTPUT_TPS='$OUTPUT_TPS' CACHED_TPS='$CACHED_TPS' COST_IN='$COST_IN' COST_OUT='$COST_OUT' COST_CACHED='$COST_CACHED'"
          
          # Set environment variables for the Python script
          export MODEL="${{ env.MODEL }}"
          export DGX_COST="${DGX_COST}"
          export FULL_IMAGE="${{ steps.vars.outputs.FULL_IMAGE }}"
          export INPUT_TPS="${INPUT_TPS}"
          export OUTPUT_TPS="${OUTPUT_TPS}"
          export CACHED_TPS="${CACHED_TPS}"
          export COST_IN="${COST_IN}"
          export COST_OUT="${COST_OUT}"
          export COST_CACHED="${COST_CACHED}"
          
          # Generate results JSON using Python script
          python3 scripts/generate_results.py
          
          # Set outputs for summary
          echo "prefill_tps=$INPUT_TPS" >> $GITHUB_OUTPUT
          echo "cached_tps=$CACHED_TPS" >> $GITHUB_OUTPUT
          echo "decode_tps=$OUTPUT_TPS" >> $GITHUB_OUTPUT
          echo "cost_in=$COST_IN" >> $GITHUB_OUTPUT
          echo "cost_cached=$COST_CACHED" >> $GITHUB_OUTPUT
          echo "cost_out=$COST_OUT" >> $GITHUB_OUTPUT

      - name: Update pricing in docs
        if: success()
        run: |
          python3 scripts/update_pricing.py \
            --results bench_results.json \
            --html docs/index.html
          
          # Also copy results to docs for GitHub Pages
          cp bench_results.json docs/benchmark-results.json
          
          # Append to benchmark history
          python3 -c "
          import json
          from datetime import datetime
          
          # Load new results
          with open('bench_results.json', 'r') as f:
              new_result = json.load(f)
          
          # Add version from image tag
          new_result['version'] = '${{ steps.vars.outputs.TAG }}'
          
          # Load existing history
          history_file = 'docs/benchmark-history.json'
          try:
              with open(history_file, 'r') as f:
                  history = json.load(f)
          except (FileNotFoundError, json.JSONDecodeError):
              history = []
          
          # Check if this version already exists (update it) or add new
          version_exists = False
          for i, entry in enumerate(history):
              if entry.get('version') == new_result['version']:
                  history[i] = new_result
                  version_exists = True
                  break
          
          if not version_exists:
              history.append(new_result)
          
          # Keep only last 50 entries
          history = history[-50:]
          
          # Save updated history
          with open(history_file, 'w') as f:
              json.dump(history, f, indent=2)
          
          print(f'History updated: {len(history)} entries')
          "

      - name: Commit and push pricing updates
        if: success()
        run: |
          cd $GITHUB_WORKSPACE
          pwd
          ls -la docs/
          git status
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          # Check if there are any changes to commit
          if git diff --quiet docs/index.html docs/benchmark-results.json docs/benchmark-history.json 2>/dev/null; then
            echo "No pricing changes to commit"
          else
            git add docs/index.html docs/benchmark-results.json docs/benchmark-history.json
            git commit -m "Update pricing and benchmark results [skip ci]"
            
            # Retry push with pull --rebase in case of concurrent updates
            MAX_RETRIES=3
            RETRY=0
            while [ $RETRY -lt $MAX_RETRIES ]; do
              if git push; then
                echo "âœ… Pricing and benchmark results updated and pushed"
                break
              else
                RETRY=$((RETRY+1))
                echo "Push failed, attempt $RETRY of $MAX_RETRIES. Pulling latest changes..."
                git pull --rebase origin main
              fi
            done
            
            if [ $RETRY -eq $MAX_RETRIES ]; then
              echo "âŒ Failed to push after $MAX_RETRIES attempts"
              exit 1
            fi
          fi

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: bench_results.json

      - name: Add summary
        run: |
          echo "## ðŸš€ Deployment and Benchmark Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Image:** \`${{ steps.vars.outputs.FULL_IMAGE }}\`" >> $GITHUB_STEP_SUMMARY
          echo "- **Model:** \`${{ env.MODEL }}\`" >> $GITHUB_STEP_SUMMARY
          echo "- **Container:** \`${{ env.CONTAINER_NAME }}\` running on port ${{ env.VLLM_PORT }}" >> $GITHUB_STEP_SUMMARY
          echo "- **DGX Spark Cost:** \`\$${{ env.DGX_COST_PER_HOUR }}/hour\`" >> $GITHUB_STEP_SUMMARY
          echo "- **LMCache:** Enabled (CPU offload)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Benchmark Results (vllm bench serve)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Prefill (Input) | Cached (Input) | Decode (Output) |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-----------------|----------------|-----------------|" >> $GITHUB_STEP_SUMMARY
          echo "| Throughput | ${{ steps.benchmark.outputs.prefill_tps }} tok/s | ${{ steps.benchmark.outputs.cached_tps }} tok/s | ${{ steps.benchmark.outputs.decode_tps }} tok/s |" >> $GITHUB_STEP_SUMMARY
          echo "| Cost per 1M tokens | \$${{ steps.benchmark.outputs.cost_in }} | \$${{ steps.benchmark.outputs.cost_cached }} | \$${{ steps.benchmark.outputs.cost_out }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f bench_results.json ]; then
            echo "### Raw Results" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`json" >> $GITHUB_STEP_SUMMARY
            cat bench_results.json >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          fi
