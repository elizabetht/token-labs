name: Deploy and Benchmark on DGX Spark

on:
  workflow_run:
    workflows: ["Build and push vLLM image"]
    types: [completed]
  workflow_dispatch:
    inputs:
      image_tag:
        description: 'Image tag to deploy (default: latest)'
        required: false
        default: 'latest'
      model:
        description: 'Model to benchmark'
        required: false
        default: 'tokenlabsdotrun/Llama-3.1-8B-ModelOpt-NVFP4'
        type: choice
        options:
          - 'meta-llama/Llama-3.1-8B-Instruct'
          - 'tokenlabsdotrun/Llama-3.1-8B-ModelOpt-NVFP4'
          - 'tokenlabsdotrun/Llama-3.1-8B-ModelOpt-FP8'
          - 'tokenlabsdotrun/Llama-3.1-8B-Unsloth-FP8-QAT'
          - 'tokenlabsdotrun/Llama-3.1-8B-ModelOpt-NVFP4-QAT'
          - 'tokenlabsdotrun/Llama-3.1-8B-ModelOpt-FP8-QAT'
      ifeval_samples:
        description: 'Number of IFEval samples (50 for quick, 541 for full)'
        required: false
        default: '50'
        type: choice
        options:
          - '50'
          - '100'
          - '200'
          - '541'
      accuracy_only:
        description: 'Run only accuracy tests (skip performance benchmarks)'
        required: false
        default: false
        type: boolean

permissions:
  contents: write
  packages: read

env:
  CONTAINER_NAME: vllm-server
  VLLM_PORT: 8000
  MODEL: ${{ github.event.inputs.model || 'tokenlabsdotrun/Llama-3.1-8B-ModelOpt-NVFP4' }}
  DRAFT_MODEL: RedHatAI/Llama-3.1-8B-Instruct-speculator.eagle3
  DGX_HARDWARE_COST_PER_HOUR: "0.152"
  DGX_ELECTRICITY_COST_PER_HOUR: "0.02"
  GPU_MEMORY_UTILIZATION: "0.6"
  DGX_TAILSCALE_IP: "100.64.38.13"

jobs:
  deploy-and-benchmark:
    runs-on: [self-hosted, DGX-Spark]
    if: ${{ github.event.workflow_run.conclusion == 'success' || github.event_name == 'workflow_dispatch' }}
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          ref: main

      - name: Determine image tag
        id: vars
        run: |
          OWNER=$(echo "${GITHUB_REPOSITORY_OWNER}" | tr '[:upper:]' '[:lower:]')
          IMAGE="ghcr.io/${OWNER}/token-labs/vllm-serve"
          TAG="${{ github.event.inputs.image_tag || github.event.workflow_run.head_branch || 'latest' }}"
          VERSION_TAG="${TAG#v}"
          
          echo "IMAGE=${IMAGE}" >> "$GITHUB_OUTPUT"
          echo "TAG=${TAG}" >> "$GITHUB_OUTPUT"
          echo "VERSION_TAG=${VERSION_TAG}" >> "$GITHUB_OUTPUT"
          echo "FULL_IMAGE=${IMAGE}:${TAG}" >> "$GITHUB_OUTPUT"

      - name: Login and pull image
        run: |
          echo "${{ secrets.GITHUB_TOKEN }}" | docker login ghcr.io -u ${{ github.actor }} --password-stdin
          docker pull ${{ steps.vars.outputs.FULL_IMAGE }} || docker pull ${{ steps.vars.outputs.IMAGE}}:${TAG#v}

      - name: Stop existing container
        run: |
          docker stop ${{ env.CONTAINER_NAME }} 2>/dev/null || true
          docker rm ${{ env.CONTAINER_NAME }} 2>/dev/null || true

      - name: Start vLLM server
        run: |
          VERSION="${{ steps.vars.outputs.VERSION_TAG }}"
          IMAGE="${FULL_IMAGE:-${{ steps.vars.outputs.FULL_IMAGE }}}"
          
          case "$VERSION" in
            0.1.0|v0.1.0)
              export LMCACHE_ENABLED="false" SPECULATIVE_DECODING_ENABLED="false" PREFIX_CACHING_ENABLED="false"
              docker run -d --gpus all --name ${{ env.CONTAINER_NAME }} \
                -p ${{ env.DGX_TAILSCALE_IP}}:${{ env.VLLM_PORT }}:8000 \
                -e HF_TOKEN=${{ secrets.HF_TOKEN }} \
                -e FLASHINFER_DISABLE_VERSION_CHECK=1 \
                -v $HOME/.cache/huggingface:/root/.cache/huggingface \
                "${IMAGE}" ${{ env.MODEL }} --gpu-memory-utilization 0.3
              ;;
            0.2.0|v0.2.0)
              export LMCACHE_ENABLED="true" SPECULATIVE_DECODING_ENABLED="false" PREFIX_CACHING_ENABLED="false"
              docker run -d --gpus all --name ${{ env.CONTAINER_NAME }} \
                -p ${{ env.DGX_TAILSCALE_IP}}:${{ env.VLLM_PORT }}:8000 \
                -e HF_TOKEN=${{ secrets.HF_TOKEN }} \
                -e FLASHINFER_DISABLE_VERSION_CHECK=1 \
                -e LMCACHE_LOG_LEVEL=WARNING \
                -e LMCACHE_CONFIG_FILE=/app/config/lmcache-cpu-offload.yaml \
                -e LMCACHE_USE_EXPERIMENTAL=True \
                -v $HOME/.cache/huggingface:/root/.cache/huggingface \
                -v ${{ github.workspace }}/config:/app/config:ro \
                "${IMAGE}" ${{ env.MODEL }} \
                --gpu-memory-utilization 0.3 \
                --kv-transfer-config '{"kv_connector":"LMCacheConnectorV1","kv_role":"kv_both"}' \
                --no-enable-prefix-caching
              ;;
            0.4.0|v0.4.0)
              export LMCACHE_ENABLED="false" SPECULATIVE_DECODING_ENABLED="false" PREFIX_CACHING_ENABLED="false"
              docker run -d --gpus all --name ${{ env.CONTAINER_NAME }} \
                -p ${{ env.DGX_TAILSCALE_IP}}:${{ env.VLLM_PORT }}:8000 \
                -e HF_TOKEN=${{ secrets.HF_TOKEN }} \
                -e FLASHINFER_DISABLE_VERSION_CHECK=1 \
                -v $HOME/.cache/huggingface:$HOME/.cache/huggingface \
                "${IMAGE}" ${{ env.MODEL }} --gpu-memory-utilization 0.3 \
                --quantization modelopt_fp4
              ;;
            *)
              export LMCACHE_ENABLED="false" SPECULATIVE_DECODING_ENABLED="true" PREFIX_CACHING_ENABLED="true"
              docker run -d --gpus all --name ${{ env.CONTAINER_NAME }} \
                -p ${{ env.DGX_TAILSCALE_IP}}:${{ env.VLLM_PORT }}:8000 \
                -e HF_TOKEN=${{ secrets.HF_TOKEN }} \
                -e FLASHINFER_DISABLE_VERSION_CHECK=1 \
                -v $HOME/.cache/huggingface:/root/.cache/huggingface \
                "${IMAGE}" ${{ env.MODEL }} \
                --gpu-memory-utilization ${{ env.GPU_MEMORY_UTILIZATION }} \
                --enable-prefix-caching \
                --speculative-config '{"model": "${{ env.DRAFT_MODEL }}", "num_speculative_tokens": 7, "method": "eagle3"}'
              ;;
          esac
          
          # Wait for server
          for i in {1..300}; do
            curl -sf http://${{ env.DGX_TAILSCALE_IP}}:${{ env.VLLM_PORT }}/health && break
            sleep 1
          done

      - name: Run IFEval accuracy evaluation
        id: ifeval
        run: |
          echo "Running IFEval accuracy benchmark with lighteval..."
          
          # Override HOME to avoid permission issues with Docker-created cache
          export HOME="${{ github.workspace }}/.lighteval_home"
          mkdir -p "$HOME/.cache/huggingface"
          
          # Verify vLLM server is reachable before starting evaluation
          echo "Checking vLLM server connectivity..."
          for i in {1..30}; do
            if curl -sf "http://${{ env.DGX_TAILSCALE_IP }}:${{ env.VLLM_PORT }}/v1/models" > /dev/null 2>&1; then
              echo "âœ“ vLLM server is ready"
              break
            fi
            echo "Waiting for vLLM server... ($i/30)"
            sleep 2
          done
          
          # Get the actual model name from vLLM
          echo "Querying vLLM for available models..."
          VLLM_MODELS=$(curl -s "http://${{ env.DGX_TAILSCALE_IP }}:${{ env.VLLM_PORT }}/v1/models")
          echo "Available models: $VLLM_MODELS"
          VLLM_MODEL_ID=$(echo "$VLLM_MODELS" | python3 -c "import sys,json; print(json.load(sys.stdin)['data'][0]['id'])" 2>/dev/null || echo "${{ env.MODEL }}")
          echo "Using model ID: $VLLM_MODEL_ID"
          
          # Test with a simple completion request
          echo "Testing completion endpoint..."
          curl -s "http://${{ env.DGX_TAILSCALE_IP }}:${{ env.VLLM_PORT }}/v1/chat/completions" \
            -H "Content-Type: application/json" \
            -d "{\"model\": \"$VLLM_MODEL_ID\", \"messages\": [{\"role\": \"user\", \"content\": \"Hi\"}], \"max_tokens\": 5}" \
            | head -c 300
          echo ""
          
          # Create virtual environment and install lighteval
          python3 -m venv .venv-ifeval
          source .venv-ifeval/bin/activate
          pip install -q --upgrade pip
          pip install -q 'lighteval[litellm]' langdetect
          
          NUM_SAMPLES="${{ github.event.inputs.ifeval_samples || '50' }}"
          
          # Run lighteval with the correct model ID
          # Use openai/ prefix for litellm to route to OpenAI-compatible endpoint
          echo "Running lighteval with model: openai/$VLLM_MODEL_ID"
          if HOME="$HOME" lighteval endpoint litellm \
            "model_name=openai/$VLLM_MODEL_ID,base_url=http://${{ env.DGX_TAILSCALE_IP }}:${{ env.VLLM_PORT }}/v1,api_key=dummy" \
            ifeval \
            --output-dir ./lighteval_results \
            --max-samples "$NUM_SAMPLES" \
            --save-details; then
            
            RESULTS_FILE=$(find ./lighteval_results -name "results_*.json" | head -1)
            if [ -n "$RESULTS_FILE" ]; then
              # Parse results
              python3 << EOF
          import json
          with open("$RESULTS_FILE") as f:
              data = json.load(f)
          results = data.get("results", {}).get("all", {})
          prompt_acc = results.get("prompt_level_strict_acc", 0) * 100
          inst_acc = results.get("inst_level_strict_acc", 0) * 100
          prompt_acc_loose = results.get("prompt_level_loose_acc", 0) * 100
          inst_acc_loose = results.get("inst_level_loose_acc", 0) * 100
          output = {
              "model": "${{ env.MODEL }}",
              "num_samples": $NUM_SAMPLES,
              "prompt_level_accuracy": prompt_acc,
              "instruction_level_accuracy": inst_acc,
              "prompt_level_accuracy_loose": prompt_acc_loose,
              "instruction_level_accuracy_loose": inst_acc_loose,
          }
          with open("ifeval_results.json", "w") as f:
              json.dump(output, f, indent=2)
          print(f"prompt_accuracy={prompt_acc}")
          print(f"instruction_accuracy={inst_acc}")
          print(f"num_samples=$NUM_SAMPLES")
          EOF
              # Set outputs
              python3 -c "import json; d=json.load(open('ifeval_results.json')); print(f\"prompt_accuracy={d['prompt_level_accuracy']}\"); print(f\"instruction_accuracy={d['instruction_level_accuracy']}\"); print(f\"num_samples={d['num_samples']}\")" >> $GITHUB_OUTPUT
            fi
          else
            echo "IFEval failed, using zero accuracy"
            echo '{"prompt_level_accuracy": 0, "instruction_level_accuracy": 0, "num_samples": 0}' > ifeval_results.json
            echo "prompt_accuracy=0" >> $GITHUB_OUTPUT
            echo "instruction_accuracy=0" >> $GITHUB_OUTPUT
            echo "num_samples=0" >> $GITHUB_OUTPUT
          fi

      - name: Compare with baseline
        id: baseline_comparison
        continue-on-error: true
        run: |
          # Determine baseline file based on model
          if [[ "${{ env.MODEL }}" == "meta-llama/Llama-3.1-8B-Instruct" ]]; then
            # For baseline model, update the baseline file
            echo "This is the baseline model. Updating baseline values..."
            if python3 scripts/compare_baseline.py \
              --results ifeval_results.json \
              --baseline baselines/llama-3.1-8b-instruct.json \
              --update-baseline \
              --run-id "${{ github.run_id }}"; then
              echo "comparison_status=BASELINE_UPDATED" >> $GITHUB_OUTPUT
            else
              echo "Failed to update baseline (exit code: $?)"
              echo "comparison_status=UPDATE_FAILED" >> $GITHUB_OUTPUT
            fi
          else
            # For other models, compare against baseline
            echo "Comparing against baseline model..."
            if python3 scripts/compare_baseline.py \
              --results ifeval_results.json \
              --baseline baselines/llama-3.1-8b-instruct.json \
              --output comparison_results.json; then
              # Comparison succeeded (model passed)
              if [ -f comparison_results.json ]; then
                COMPARISON_STATUS=$(python3 -c "import json; print(json.load(open('comparison_results.json')).get('status', 'UNKNOWN'))")
                echo "comparison_status=$COMPARISON_STATUS" >> $GITHUB_OUTPUT
                cat comparison_results.json
              fi
            else
              EXIT_CODE=$?
              # Exit code 1 means comparison failed (model degraded)
              # Other exit codes indicate script errors
              if [ $EXIT_CODE -eq 1 ]; then
                echo "Comparison completed: model accuracy degraded beyond threshold"
                if [ -f comparison_results.json ]; then
                  COMPARISON_STATUS=$(python3 -c "import json; print(json.load(open('comparison_results.json')).get('status', 'UNKNOWN'))")
                  echo "comparison_status=$COMPARISON_STATUS" >> $GITHUB_OUTPUT
                  cat comparison_results.json
                else
                  echo "comparison_status=FAILED" >> $GITHUB_OUTPUT
                fi
              else
                echo "Error running comparison script (exit code: $EXIT_CODE)"
                echo "comparison_status=ERROR" >> $GITHUB_OUTPUT
              fi
            fi
          fi

      - name: Run benchmarks
        id: benchmark
        if: ${{ github.event.inputs.accuracy_only != 'true' }}
        run: |
          VERSION="${{ steps.vars.outputs.VERSION_TAG }}"
          
          # Calculate cost
          GPU_UTIL=${{ env.GPU_MEMORY_UTILIZATION }}
          DGX_COST=$(echo "scale=4; (${{ env.DGX_HARDWARE_COST_PER_HOUR }} * $GPU_UTIL) + ${{ env.DGX_ELECTRICITY_COST_PER_HOUR }}" | bc)
          
          # Run benchmarks sequentially to avoid issues
          # Skip cache benchmark for v0.1.0 as it doesn't have prefix caching
          if [[ "$VERSION" != "0.1.0" && "$VERSION" != "v0.1.0" ]]; then
            echo "Running cache benchmark with guidellm..."
            docker exec ${{ env.CONTAINER_NAME }} bash -c "source /opt/venv/bin/activate && \
              FLASHINFER_DISABLE_VERSION_CHECK=1 guidellm benchmark \
              --target http://${{ env.DGX_TAILSCALE_IP }}:${{ env.VLLM_PORT }} \
              --profile constant \
              --rate 10 \
              --max-requests 100 \
              --data 'prompt_tokens=512,output_tokens=128' \
              --output-path /tmp/cache_bench" \
              2>&1 | tee cache_bench.txt
            
            # Copy benchmark results from container
            docker cp ${{ env.CONTAINER_NAME }}:/tmp/cache_bench/benchmarks.json cache_bench.json || echo '{}' > cache_bench.json
          else
            echo "Skipping cache benchmark for v0.1.0 (no prefix caching support)"
            echo '{}' > cache_bench.json
          fi
          
          echo "Running prefill benchmark with guidellm..."
          docker exec ${{ env.CONTAINER_NAME }} bash -c "source /opt/venv/bin/activate && \
            FLASHINFER_DISABLE_VERSION_CHECK=1 guidellm benchmark \
            --target http://${{ env.DGX_TAILSCALE_IP }}:${{ env.VLLM_PORT }} \
            --profile constant \
            --rate 10 \
            --max-requests 100 \
            --data 'prompt_tokens=3072,output_tokens=1024' \
            --output-path /tmp/prefill_bench" \
            2>&1 | tee prefill_bench.txt
          
          # Copy benchmark results from container
          docker cp ${{ env.CONTAINER_NAME }}:/tmp/prefill_bench/benchmarks.json prefill_bench.json || echo '{}' > prefill_bench.json
          
          echo "Running decode benchmark with guidellm..."
          docker exec ${{ env.CONTAINER_NAME }} bash -c "source /opt/venv/bin/activate && \
            FLASHINFER_DISABLE_VERSION_CHECK=1 guidellm benchmark \
            --target http://${{ env.DGX_TAILSCALE_IP }}:${{ env.VLLM_PORT }} \
            --profile constant \
            --rate 10 \
            --max-requests 100 \
            --data 'prompt_tokens=1024,output_tokens=3072' \
            --output-path /tmp/decode_bench" \
            2>&1 | tee decode_bench.txt
          
          # Copy benchmark results from container
          docker cp ${{ env.CONTAINER_NAME }}:/tmp/decode_bench/benchmarks.json decode_bench.json || echo '{}' > decode_bench.json
          
          # Parse guidellm JSON output and extract metrics
          cat > /tmp/parse_guidellm.py << 'PYEOF'
          import json
          import sys
          
          def extract_metrics(json_file):
              """Extract metrics from guidellm JSON output"""
              try:
                  with open(json_file) as f:
                      data = json.load(f)
                  
                  # Debug: print structure
                  print(f"DEBUG: {json_file} top-level keys: {list(data.keys())}", file=sys.stderr)
                  
                  # guidellm outputs a dict with benchmarks list
                  if not data:
                      return {}
                  
                  benchmarks = data.get('benchmarks', [])
                  if not benchmarks:
                      print(f"DEBUG: {json_file} has no benchmarks", file=sys.stderr)
                      return {}
                  
                  # Get the first benchmark (we only run one per file)
                  benchmark = benchmarks[0] if isinstance(benchmarks, list) else benchmarks
                  print(f"DEBUG: {json_file} benchmark keys: {list(benchmark.keys())}", file=sys.stderr)
                  
                  # Try different possible paths to stats
                  stats = benchmark.get('report', benchmark.get('statistics', benchmark.get('stats', {})))
                  print(f"DEBUG: {json_file} stats keys: {list(stats.keys()) if isinstance(stats, dict) else 'not a dict'}", file=sys.stderr)
                  
                  # Helper to get value with fallback field names
                  def get_with_fallback(field_names, default=0):
                      for field in field_names:
                          val = stats.get(field)
                          if val is not None:
                              return val
                      return default
                  
                  # Extract throughput metrics with fallbacks for different field names
                  prompt_token_throughput = get_with_fallback([
                      'prompt_token_throughput',
                      'prompt_tokens_per_second', 
                      'input_token_throughput'
                  ])
                  output_token_throughput = get_with_fallback([
                      'output_token_throughput',
                      'output_tokens_per_second',
                      'decode_token_throughput'
                  ])
                  total_token_throughput = get_with_fallback([
                      'total_token_throughput',
                      'total_tokens_per_second',
                      'throughput'
                  ])
                  
                  # Extract latency metrics - guidellm outputs in seconds, convert to ms
                  # Based on guidellm metrics documentation, all latency values are in seconds
                  def get_latency(name):
                      # Try primary field name
                      val = stats.get(name)
                      # Always convert from seconds to milliseconds if value exists
                      return (val * 1000) if val is not None else 0
                  
                  ttft_mean = get_latency('ttft_mean')
                  ttft_median = get_latency('ttft_median')
                  ttft_p99 = get_latency('ttft_p99')
                  
                  tpot_mean = get_latency('tpot_mean')
                  tpot_median = get_latency('tpot_median')
                  tpot_p99 = get_latency('tpot_p99')
                  
                  itl_mean = get_latency('itl_mean')
                  itl_median = get_latency('itl_median')
                  itl_p99 = get_latency('itl_p99')
                  
                  return {
                      'prompt_token_throughput': prompt_token_throughput,
                      'output_token_throughput': output_token_throughput,
                      'total_token_throughput': total_token_throughput,
                      'ttft_mean': ttft_mean,
                      'ttft_median': ttft_median,
                      'ttft_p99': ttft_p99,
                      'tpot_mean': tpot_mean,
                      'tpot_median': tpot_median,
                      'tpot_p99': tpot_p99,
                      'itl_mean': itl_mean,
                      'itl_median': itl_median,
                      'itl_p99': itl_p99
                  }
              except Exception as e:
                  print(f"Error parsing {json_file}: {e}", file=sys.stderr)
                  import traceback
                  traceback.print_exc(file=sys.stderr)
                  return {}
          
          # Parse each benchmark file
          prefill = extract_metrics('prefill_bench.json')
          decode = extract_metrics('decode_bench.json')
          cache = extract_metrics('cache_bench.json')
          
          # Output in shell variable format
          print(f"INPUT_TPS={prefill.get('prompt_token_throughput', 0)}")
          print(f"OUTPUT_TPS={decode.get('output_token_throughput', 0)}")
          print(f"CACHED_TPS={cache.get('total_token_throughput', 0)}")
          
          # Export latency metrics for prefill
          for key, val in prefill.items():
              if key.startswith(('ttft_', 'tpot_', 'itl_')):
                  print(f"PREFILL_{key.upper()}={val}")
          
          # Export latency metrics for decode
          for key, val in decode.items():
              if key.startswith(('ttft_', 'tpot_', 'itl_')):
                  print(f"DECODE_{key.upper()}={val}")
          
          # Export latency metrics for cache
          for key, val in cache.items():
              if key.startswith(('ttft_', 'tpot_', 'itl_')):
                  print(f"CACHE_{key.upper()}={val}")
          PYEOF
          
          # Run parser and source the output
          python3 /tmp/parse_guidellm.py > /tmp/metrics.env
          source /tmp/metrics.env
          
          # Default to 0 if still empty
          INPUT_TPS="${INPUT_TPS:-0}"
          OUTPUT_TPS="${OUTPUT_TPS:-0}"
          CACHED_TPS="${CACHED_TPS:-0}"
          
          echo "Extracted throughput: INPUT=$INPUT_TPS OUTPUT=$OUTPUT_TPS CACHED=$CACHED_TPS"
          
          # Calculate costs (handle division by zero)
          calc_cost() { 
            if [ "$1" != "0" ] && [ -n "$1" ]; then
              echo "scale=4; $DGX_COST * 1000000 / ($1 * 3600)" | bc | awk '{printf "%.4f", $0}'
            else
              echo "0"
            fi
          }
          
          COST_IN=$(calc_cost $INPUT_TPS)
          COST_OUT=$(calc_cost $OUTPUT_TPS)
          COST_CACHED=$(calc_cost $CACHED_TPS)
          
          # Export for Python script
          export MODEL="${{ env.MODEL }}" 
          export DGX_COST 
          export FULL_IMAGE="${{ steps.vars.outputs.FULL_IMAGE }}"
          export GPU_MEMORY_UTILIZATION="${{ env.GPU_MEMORY_UTILIZATION }}"
          export INPUT_TPS OUTPUT_TPS CACHED_TPS COST_IN COST_OUT COST_CACHED
          export PREFILL_NUM_PROMPTS=100 PREFILL_REQUEST_RATE=10 PREFILL_INPUT_LEN=3072 PREFILL_OUTPUT_LEN=1024
          export DECODE_NUM_PROMPTS=100 DECODE_REQUEST_RATE=10 DECODE_INPUT_LEN=1024 DECODE_OUTPUT_LEN=3072
          export CACHE_NUM_PROMPTS=100 CACHE_PREFIX_LEN=512 CACHE_SUFFIX_LEN=128 CACHE_NUM_PREFIXES=5 CACHE_OUTPUT_LEN=128
          
          python3 scripts/generate_results.py
          
          # Set outputs
          echo "prefill_tps=$INPUT_TPS" >> $GITHUB_OUTPUT
          echo "decode_tps=$OUTPUT_TPS" >> $GITHUB_OUTPUT
          echo "cached_tps=$CACHED_TPS" >> $GITHUB_OUTPUT
          echo "cost_in=$COST_IN" >> $GITHUB_OUTPUT
          echo "cost_out=$COST_OUT" >> $GITHUB_OUTPUT
          echo "cost_cached=$COST_CACHED" >> $GITHUB_OUTPUT

      - name: Generate final results
        run: |
          # Re-run generate_results with accuracy metrics
          export MODEL="${{ env.MODEL }}"
          export DGX_COST=$(echo "scale=4; (${{ env.DGX_HARDWARE_COST_PER_HOUR }} * ${{ env.GPU_MEMORY_UTILIZATION }}) + ${{ env.DGX_ELECTRICITY_COST_PER_HOUR }}" | bc)
          export FULL_IMAGE="${{ steps.vars.outputs.FULL_IMAGE }}"
          export GPU_MEMORY_UTILIZATION="${{ env.GPU_MEMORY_UTILIZATION }}"
          
          # Only include benchmark metrics if benchmarks were run
          if [[ "${{ github.event.inputs.accuracy_only }}" != "true" ]]; then
            export INPUT_TPS="${{ steps.benchmark.outputs.prefill_tps }}"
            export OUTPUT_TPS="${{ steps.benchmark.outputs.decode_tps }}"
            export CACHED_TPS="${{ steps.benchmark.outputs.cached_tps }}"
            export COST_IN="${{ steps.benchmark.outputs.cost_in }}"
            export COST_OUT="${{ steps.benchmark.outputs.cost_out }}"
            export COST_CACHED="${{ steps.benchmark.outputs.cost_cached }}"
            export PREFILL_NUM_PROMPTS=100 PREFILL_REQUEST_RATE=10 PREFILL_INPUT_LEN=3072 PREFILL_OUTPUT_LEN=1024
            export DECODE_NUM_PROMPTS=100 DECODE_REQUEST_RATE=10 DECODE_INPUT_LEN=1024 DECODE_OUTPUT_LEN=3072
            export CACHE_NUM_PROMPTS=100 CACHE_PREFIX_LEN=512 CACHE_SUFFIX_LEN=128 CACHE_NUM_PREFIXES=5 CACHE_OUTPUT_LEN=128
            export ACCURACY_ONLY="false"
          else
            export INPUT_TPS="0"
            export OUTPUT_TPS="0"
            export CACHED_TPS="0"
            export COST_IN="0"
            export COST_OUT="0"
            export COST_CACHED="0"
            export ACCURACY_ONLY="true"
          fi
          
          export IFEVAL_PROMPT_ACCURACY="${{ steps.ifeval.outputs.prompt_accuracy }}"
          export IFEVAL_INSTRUCTION_ACCURACY="${{ steps.ifeval.outputs.instruction_accuracy }}"
          export IFEVAL_NUM_SAMPLES="${{ steps.ifeval.outputs.num_samples }}"
          export IFEVAL_EVALUATED="true"
          
          python3 scripts/generate_results.py

      - name: Update docs
        if: success()
        run: |
          python3 scripts/update_pricing.py --results bench_results.json --html docs/index.html
          cp bench_results.json docs/benchmark-results.json
          cp ifeval_results.json docs/ifeval-results.json
          # Copy comparison results if available
          if [ -f comparison_results.json ]; then
            cp comparison_results.json docs/comparison-results.json
          fi
          
          # Update history
          python3 -c "
          import json, re, os
          with open('bench_results.json') as f: new_result = json.load(f)
          version = '${{ steps.vars.outputs.VERSION_TAG }}'
          model = '${{ env.MODEL }}'
          accuracy_only = '${{ github.event.inputs.accuracy_only }}' == 'true'
          new_result['version'] = version
          
          if not re.search(r'\b(rc|beta|alpha|dev|pre|snapshot|latest)\b', version, re.I):
              # Update benchmark history (only if not accuracy_only mode)
              if not accuracy_only:
                  try:
                      with open('docs/benchmark-history.json') as f: history = json.load(f)
                  except: history = []
                  
                  # Use version + model as composite key to allow multiple models per version
                  for i, e in enumerate(history):
                      if e.get('version') == version and e.get('model') == model:
                          history[i] = new_result
                          break
                  else:
                      history.append(new_result)
                  
                  with open('docs/benchmark-history.json', 'w') as f:
                      json.dump(history[-50:], f, indent=2)
              
              # Update accuracy history (only entries with accuracy data)
              if new_result.get('accuracy') and new_result['accuracy'].get('ifeval') and new_result['accuracy']['ifeval'].get('evaluated'):
                  try:
                      with open('docs/accuracy-history.json') as f: acc_history = json.load(f)
                  except: acc_history = []
                  
                  # Extract only accuracy-related fields
                  acc_entry = {
                      'model': new_result.get('model'),
                      'version': new_result.get('version'),
                      'timestamp': new_result.get('timestamp'),
                      'image_tag': new_result.get('image_tag'),
                      'accuracy': new_result.get('accuracy')
                  }
                  
                  # Use version + model as composite key
                  for i, e in enumerate(acc_history):
                      if e.get('version') == version and e.get('model') == model:
                          acc_history[i] = acc_entry
                          break
                  else:
                      acc_history.append(acc_entry)
                  
                  with open('docs/accuracy-history.json', 'w') as f:
                      json.dump(acc_history[-50:], f, indent=2)
          "

      - name: Commit and push
        if: success()
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add docs/*.{html,json} 2>/dev/null || true
          # Only add the specific baseline file for Llama 3.1 8B
          git add baselines/llama-3.1-8b-instruct.json 2>/dev/null || true
          git add comparison_results.json 2>/dev/null || true
          git diff --quiet --cached || {
            git commit -m "Update benchmark results [skip ci]"
            for i in {1..3}; do
              git push origin main && break || git pull --rebase origin main
            done
          }

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: |
            bench_results.json
            ifeval_results.json
            comparison_results.json

      - name: Summary
        run: |
          DGX_COST=$(echo "scale=4; (${{ env.DGX_HARDWARE_COST_PER_HOUR }} * ${{ env.GPU_MEMORY_UTILIZATION }}) + ${{ env.DGX_ELECTRICITY_COST_PER_HOUR }}" | bc)
          
          cat >> $GITHUB_STEP_SUMMARY << EOF
          ## ðŸš€ Benchmark Complete
          
          **Version:** \`${{ steps.vars.outputs.VERSION_TAG }}\`
          **Cost:** \`\$${DGX_COST}/hr\` (GPU: ${{ env.GPU_MEMORY_UTILIZATION }})
          
          ### Performance
          | Metric | Prefill | Cached | Decode |
          |--------|---------|--------|--------|
          | **Throughput (tok/s)** | ${{ steps.benchmark.outputs.prefill_tps }} | ${{ steps.benchmark.outputs.cached_tps }} | ${{ steps.benchmark.outputs.decode_tps }} |
          | **Cost per 1M tokens** | \$${{ steps.benchmark.outputs.cost_in }} | \$${{ steps.benchmark.outputs.cost_cached }} | \$${{ steps.benchmark.outputs.cost_out }} |
          
          ### Accuracy (IFEval)
          | Metric | Score |
          |--------|-------|
          | **Prompt-level accuracy** | ${{ steps.ifeval.outputs.prompt_accuracy }}% |
          | **Instruction-level accuracy** | ${{ steps.ifeval.outputs.instruction_accuracy }}% |
          | **Samples evaluated** | ${{ steps.ifeval.outputs.num_samples }} |
          
          ### Baseline Comparison
          **Status:** \`${{ steps.baseline_comparison.outputs.comparison_status || 'NOT_RUN' }}\`
          EOF
