name: Deploy and Benchmark on DGX Spark

on:
  workflow_run:
    workflows: ["Build and push vLLM image"]
    types: [completed]
  workflow_dispatch:
    inputs:
      image_tag:
        description: 'Image tag to deploy (default: latest)'
        required: false
        default: 'latest'

permissions:
  contents: write
  packages: read

env:
  CONTAINER_NAME: vllm-server
  VLLM_PORT: 8000
  MODEL: meta-llama/Llama-3.1-8B-Instruct
  # DGX Spark economics: $4000 hardware / 26280 hours (3yr) with 0.3 utilization = $0.05/hr + ~$0.02/hr electricity = $0.07/hr
  DGX_COST_PER_HOUR: "0.07"
  DGX_TAILSCALE_IP: "100.64.38.13"
  # LMCache settings
  LMCACHE_ENABLED: "true"

jobs:
  deploy-and-benchmark:
    runs-on: [self-hosted, DGX-Spark]
    # Only run if build succeeded or manually triggered
    if: ${{ github.event.workflow_run.conclusion == 'success' || github.event_name == 'workflow_dispatch' }}
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Fix Git ownership
        run: git config --global --add safe.directory "$GITHUB_WORKSPACE"

      - name: Determine image tag
        id: vars
        run: |
          OWNER=$(echo "${GITHUB_REPOSITORY_OWNER}" | tr '[:upper:]' '[:lower:]')
          IMAGE="ghcr.io/${OWNER}/token-labs/vllm-serve"
          
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            TAG="${{ github.event.inputs.image_tag }}"
          else
            # Get version from the tag that triggered the build workflow
            # workflow_run event provides head_branch which contains the tag name
            TAG="${{ github.event.workflow_run.head_branch }}"
            # Remove 'v' prefix if present (v0.1.0 -> 0.1.0)
            TAG="${TAG#v}"
            # Fallback to latest if tag is empty
            if [ -z "$TAG" ]; then
              TAG="latest"
            fi
          fi
          
          echo "IMAGE=${IMAGE}" >> "$GITHUB_OUTPUT"
          echo "TAG=${TAG}" >> "$GITHUB_OUTPUT"
          echo "FULL_IMAGE=${IMAGE}:${TAG}" >> "$GITHUB_OUTPUT"
          echo "Using image tag: $TAG"

      - name: Log in to GHCR
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Pull latest image
        run: |
          echo "Pulling image: ${{ steps.vars.outputs.FULL_IMAGE }}"
          docker pull ${{ steps.vars.outputs.FULL_IMAGE }}

      - name: Stop existing vLLM container (if running)
        run: |
          if docker ps -q -f name=${{ env.CONTAINER_NAME }} | grep -q .; then
            echo "Stopping existing container..."
            docker stop ${{ env.CONTAINER_NAME }}
          fi
          if docker ps -aq -f name=${{ env.CONTAINER_NAME }} | grep -q .; then
            echo "Removing existing container..."
            docker rm ${{ env.CONTAINER_NAME }}
          fi

      - name: Start vLLM server
        run: |
          echo "Starting vLLM server with model: ${{ env.MODEL }} (LMCache enabled)"
          docker run -d \
            --gpus all \
            --name ${{ env.CONTAINER_NAME }} \
            -p ${{ env.DGX_TAILSCALE_IP}}:${{ env.VLLM_PORT }}:8000 \
            -e HF_TOKEN=${{ secrets.HF_TOKEN }} \
            -e FLASHINFER_DISABLE_VERSION_CHECK=1 \
            -e LMCACHE_CONFIG_FILE=/app/config/lmcache-cpu-offload.yaml \
            -e LMCACHE_USE_EXPERIMENTAL=True \
            -v $HOME/.cache/huggingface:/root/.cache/huggingface \
            -v ${{ github.workspace }}/config:/app/config:ro \
            ${{ steps.vars.outputs.FULL_IMAGE }} \
            ${{ env.MODEL }} \
            --kv-transfer-config '{"kv_connector":"LMCacheConnectorV1","kv_role":"kv_both"}' \
            --gpu-memory-utilization 0.3 \
            --no-enable-prefix-caching

      - name: Wait for vLLM server to be ready
        run: |
          echo "Waiting for vLLM server to be ready..."
          MAX_ATTEMPTS=300
          ATTEMPT=0
          while [ $ATTEMPT -lt $MAX_ATTEMPTS ]; do
            if curl -s http://${{ env.DGX_TAILSCALE_IP}}:${{ env.VLLM_PORT }}/health > /dev/null 2>&1; then
              echo "âœ… vLLM server is ready!"
              break
            fi
            ATTEMPT=$((ATTEMPT+1))
            sleep 1
          done
          if [ $ATTEMPT -eq $MAX_ATTEMPTS ]; then
            echo "âŒ Server failed to start within timeout"
            docker logs ${{ env.CONTAINER_NAME }} --tail 100
            exit 1
          fi

      - name: Run vLLM serving benchmark
        id: benchmark
        run: |
          # Run separate benchmarks for prefill-heavy and decode-heavy workloads
          echo "Running vLLM serving benchmarks..."
          DGX_COST=${{ env.DGX_COST_PER_HOUR }}
          
          # echo "========================================"
          # # === PREFILL-HEAVY BENCHMARK ===
          # # 8:1 input:output ratio to measure prefill (input) performance
          # # 8192 input + 1024 output = 9216 total tokens per request
          # echo "=== PREFILL-HEAVY BENCHMARK (8:1 input:output ratio) ===="
          # docker exec ${{ env.CONTAINER_NAME }} \
          #   bash -c "source /opt/venv/bin/activate && \
          #   FLASHINFER_DISABLE_VERSION_CHECK=1 \
          #   vllm bench serve \
          #   --model ${{ env.MODEL }} \
          #   --base-url http://${{ env.DGX_TAILSCALE_IP }}:${{ env.VLLM_PORT }} \
          #   --num-prompts 100 \
          #   --request-rate 10 \
          #   --random-input-len 8192 \
          #   --random-output-len 1024" \
          #   2>&1 | tee prefill_bench.txt
          
          # cat prefill_bench.txt
          
          # # Extract prefill throughput (input tokens/s)
          # PREFILL_TPS=$(grep -oP 'Input throughput \(tok/s\):\s*\K[\d.]+' prefill_bench.txt | tail -1 || echo "0")
          # if [ "$PREFILL_TPS" = "0" ] || [ -z "$PREFILL_TPS" ]; then
          #   # Fallback to total throughput if input not found
          #   PREFILL_TPS=$(grep -oP 'Total Token throughput \(tok/s\):\s*\K[\d.]+' prefill_bench.txt | tail -1 || echo "0")
          # fi
          # echo "Prefill throughput: $PREFILL_TPS tokens/s"
          
          # echo "========================================"
          # # === DECODE-HEAVY BENCHMARK ===
          # # 1:8 input:output ratio to measure decode (output) performance
          # # 1024 input + 8192 output = 9216 total tokens per request
          # echo ""
          # echo "=== DECODE-HEAVY BENCHMARK (1:8 input:output ratio) ===="
          # docker exec ${{ env.CONTAINER_NAME }} \
          #   bash -c "source /opt/venv/bin/activate && \
          #   FLASHINFER_DISABLE_VERSION_CHECK=1 \
          #   vllm bench serve \
          #   --model ${{ env.MODEL }} \
          #   --base-url http://${{ env.DGX_TAILSCALE_IP }}:${{ env.VLLM_PORT }} \
          #   --num-prompts 100 \
          #   --request-rate 10 \
          #   --random-input-len 1024 \
          #   --random-output-len 8192" \
          #   2>&1 | tee decode_bench.txt
          
          # cat decode_bench.txt
          
          # # Extract decode throughput (output tokens/s)
          # DECODE_TPS=$(grep -oP 'Output throughput \(tok/s\):\s*\K[\d.]+' decode_bench.txt | tail -1 || echo "0")
          # if [ "$DECODE_TPS" = "0" ] || [ -z "$DECODE_TPS" ]; then
          #   # Fallback to total throughput if output not found
          #   DECODE_TPS=$(grep -oP 'Total Token throughput \(tok/s\):\s*\K[\d.]+' decode_bench.txt | tail -1 || echo "0")
          # fi
          # echo "Decode throughput: $DECODE_TPS tokens/s"
          
          echo "========================================"
          # === PREFIX CACHE BENCHMARK ===
          # Uses prefix_repetition dataset to test LMCache effectiveness
          # 5 unique prefixes repeated across 100 prompts = high cache hit potential
          echo ""
          echo "=== PREFIX CACHE BENCHMARK (LMCache test with repeated prefixes) ===="
          docker exec ${{ env.CONTAINER_NAME }} \
            bash -c "source /opt/venv/bin/activate && \
            FLASHINFER_DISABLE_VERSION_CHECK=1 \
            vllm bench serve \
            --model ${{ env.MODEL }} \
            --base-url http://${{ env.DGX_TAILSCALE_IP }}:${{ env.VLLM_PORT }} \
            --dataset-name prefix_repetition \
            --num-prompts 10 \
            --prefix-repetition-prefix-len 512 \
            --prefix-repetition-suffix-len 128 \
            --prefix-repetition-num-prefixes 5 \
            --prefix-repetition-output-len 128" \
            2>&1 | tee cache_bench.txt
          
          cat cache_bench.txt
          
          # Extract cached throughput from prefix_repetition benchmark
          CACHE_TEST_TPS=$(grep -oP 'Cached input throughput \(tok/s\):\s*\K[\d.]+' cache_bench.txt | tail -1 || echo "0")
          if [ "$CACHE_TEST_TPS" = "0" ] || [ -z "$CACHE_TEST_TPS" ]; then
            # Fallback to input throughput
            CACHE_TEST_TPS=$(grep -oP 'Input throughput \(tok/s\):\s*\K[\d.]+' cache_bench.txt | tail -1 || echo "0")
          fi
          echo "Cache test throughput: $CACHE_TEST_TPS tokens/s"
          
          # Extract prefix cache hit rate if available
          CACHE_HIT_RATE=$(grep -oP 'Prefix cache hit rate:\s*\K[\d.]+' cache_bench.txt | tail -1 || echo "0")
          echo "Prefix cache hit rate: $CACHE_HIT_RATE%"
          
          # Calculate cached input throughput using server metrics formula:
          # Cached input throughput = Avg prompt throughput Ã— Prefix cache hit rate
          echo "========================================"
          echo ""
          echo "=== EXTRACTING SERVER METRICS FOR CACHED THROUGHPUT CALCULATION ==="
          
          # Get recent server logs to extract cache hit information
          docker logs ${{ env.CONTAINER_NAME }} --tail 200 > server_logs.txt
          
          # Calculate cache hit rate from LMCache logs
          # Count cache hits vs total requests from LMCache logs
          TOTAL_REQUESTS=$(grep -c "LMCache hit tokens:" server_logs.txt || echo "0")
          CACHE_HITS=$(grep "LMCache hit tokens:" server_logs.txt | grep -v "LMCache hit tokens: 0" | wc -l || echo "0")
          
          if [ "$TOTAL_REQUESTS" != "0" ] && [ "$TOTAL_REQUESTS" -gt "0" ]; then
            SERVER_CACHE_HIT_RATE=$(echo "scale=1; ($CACHE_HITS * 100) / $TOTAL_REQUESTS" | bc)
          else
            SERVER_CACHE_HIT_RATE="$CACHE_HIT_RATE"
          fi
          
          echo "LMCache analysis: $CACHE_HITS hits out of $TOTAL_REQUESTS requests"
          echo "Calculated cache hit rate: $SERVER_CACHE_HIT_RATE%"
          
          # Use benchmark prompt throughput for calculation
          SERVER_PROMPT_TPS=$PREFILL_TPS
          echo "Using benchmark prompt throughput: $SERVER_PROMPT_TPS tokens/s"
          echo "Server cache hit rate: $SERVER_CACHE_HIT_RATE%"
          
          # Calculate cached input throughput using the formula:
          # cached_input_throughput = prompt_throughput Ã— (cache_hit_rate / 100)
          if [ "$SERVER_PROMPT_TPS" != "0" ] && [ "$SERVER_CACHE_HIT_RATE" != "0" ]; then
            CALCULATED_CACHED_TPS=$(echo "scale=2; $SERVER_PROMPT_TPS * ($SERVER_CACHE_HIT_RATE / 100)" | bc)
            echo "Calculated cached input throughput: $CALCULATED_CACHED_TPS tokens/s"
            echo "(Formula: $SERVER_PROMPT_TPS Ã— $SERVER_CACHE_HIT_RATE% = $CALCULATED_CACHED_TPS tokens/s)"
          else
            CALCULATED_CACHED_TPS="0"
            echo "Cannot calculate cached throughput: insufficient server metrics"
          fi
          
          # Use the better of benchmark result or calculated result
          if [ "$CACHE_TEST_TPS" != "0" ] && [ -n "$CACHE_TEST_TPS" ]; then
            CACHED_TPS=$CACHE_TEST_TPS
            echo "Using benchmark cached throughput: $CACHED_TPS tokens/s"
          elif [ "$CALCULATED_CACHED_TPS" != "0" ] && [ -n "$CALCULATED_CACHED_TPS" ]; then
            CACHED_TPS=$CALCULATED_CACHED_TPS
            echo "Using calculated cached throughput: $CACHED_TPS tokens/s"
          fi
          
          # Use extracted values
          INPUT_TPS=$PREFILL_TPS
          OUTPUT_TPS=$DECODE_TPS
          
          echo "========================================"
          echo ""
          echo "=== FINAL RESULTS ==="
          echo "Input (prefill) throughput: $INPUT_TPS tokens/s"
          echo "Cached input throughput: $CACHED_TPS tokens/s"
          echo "Output (decode) throughput: $OUTPUT_TPS tokens/s"
          
          # Calculate cost per 1M tokens: (cost_per_hour / tokens_per_sec) * 1_000_000 / 3600
          
          if [ "$INPUT_TPS" != "0" ] && [ -n "$INPUT_TPS" ]; then
            # Use awk to ensure proper decimal formatting (bc outputs .xxx instead of 0.xxx)
            COST_IN=$(echo "scale=4; $DGX_COST * 1000000 / ($INPUT_TPS * 3600)" | bc | awk '{printf "%.4f", $0}')
          else
            COST_IN="0"
          fi
          
          if [ "$OUTPUT_TPS" != "0" ] && [ -n "$OUTPUT_TPS" ]; then
            COST_OUT=$(echo "scale=4; $DGX_COST * 1000000 / ($OUTPUT_TPS * 3600)" | bc | awk '{printf "%.4f", $0}')
          else
            COST_OUT="0"
          fi
          
          # Calculate cost for cached tokens (should be cheaper)
          if [ "$CACHED_TPS" != "0" ] && [ -n "$CACHED_TPS" ]; then
            COST_CACHED=$(echo "scale=4; $DGX_COST * 1000000 / ($CACHED_TPS * 3600)" | bc | awk '{printf "%.4f", $0}')
          else
            COST_CACHED="0"
          fi
          
          echo "Cost per 1M input tokens: \$$COST_IN"
          echo "Cost per 1M cached tokens: \$$COST_CACHED"
          echo "Cost per 1M output tokens: \$$COST_OUT"
          
          # Ensure variables have valid numeric values for JSON
          # Default to 0 if empty or unset
          INPUT_TPS="${INPUT_TPS:-0}"
          OUTPUT_TPS="${OUTPUT_TPS:-0}"
          CACHED_TPS="${CACHED_TPS:-0}"
          COST_IN="${COST_IN:-0}"
          COST_OUT="${COST_OUT:-0}"
          COST_CACHED="${COST_CACHED:-0}"
          
          # Handle empty strings explicitly
          [[ -z "$INPUT_TPS" || "$INPUT_TPS" == "" ]] && INPUT_TPS="0"
          [[ -z "$OUTPUT_TPS" || "$OUTPUT_TPS" == "" ]] && OUTPUT_TPS="0"
          [[ -z "$CACHED_TPS" || "$CACHED_TPS" == "" ]] && CACHED_TPS="0"
          [[ -z "$COST_IN" || "$COST_IN" == "" ]] && COST_IN="0"
          [[ -z "$COST_OUT" || "$COST_OUT" == "" ]] && COST_OUT="0"
          [[ -z "$COST_CACHED" || "$COST_CACHED" == "" ]] && COST_CACHED="0"
          
          # Debug: print values before JSON generation
          echo "DEBUG: INPUT_TPS='$INPUT_TPS' OUTPUT_TPS='$OUTPUT_TPS' CACHED_TPS='$CACHED_TPS' COST_IN='$COST_IN' COST_OUT='$COST_OUT' COST_CACHED='$COST_CACHED'"
          
          # Set environment variables for the Python script
          export MODEL="${{ env.MODEL }}"
          export DGX_COST="${DGX_COST}"
          export FULL_IMAGE="${{ steps.vars.outputs.FULL_IMAGE }}"
          export INPUT_TPS="${INPUT_TPS}"
          export OUTPUT_TPS="${OUTPUT_TPS}"
          export CACHED_TPS="${CACHED_TPS}"
          export COST_IN="${COST_IN}"
          export COST_OUT="${COST_OUT}"
          export COST_CACHED="${COST_CACHED}"
          
          # Generate results JSON using Python script
          python3 scripts/generate_results.py
          
          # Set outputs for summary
          echo "prefill_tps=$INPUT_TPS" >> $GITHUB_OUTPUT
          echo "cached_tps=$CACHED_TPS" >> $GITHUB_OUTPUT
          echo "decode_tps=$OUTPUT_TPS" >> $GITHUB_OUTPUT
          echo "cost_in=$COST_IN" >> $GITHUB_OUTPUT
          echo "cost_cached=$COST_CACHED" >> $GITHUB_OUTPUT
          echo "cost_out=$COST_OUT" >> $GITHUB_OUTPUT

      - name: Update pricing in docs
        if: success()
        run: |
          python3 scripts/update_pricing.py \
            --results bench_results.json \
            --html docs/index.html
          
          # Also copy results to docs for GitHub Pages
          cp bench_results.json docs/benchmark-results.json
          
          # Append to benchmark history
          python3 -c "
          import json
          import re
          from datetime import datetime
          
          # Load new results
          with open('bench_results.json', 'r') as f:
              new_result = json.load(f)
          
          # Add version from image tag
          version_tag = '${{ steps.vars.outputs.TAG }}'
          new_result['version'] = version_tag
          
          # Only add to history for stable releases (not pre-releases)
          # Skip versions with: rc, beta, alpha, dev, pre, snapshot
          is_prerelease = bool(re.search(r'\b(rc|beta|alpha|dev|pre|snapshot)\b', version_tag, re.IGNORECASE))
          is_latest = version_tag == 'latest'
          
          print(f'Version: {version_tag}, Is pre-release: {is_prerelease}, Is latest: {is_latest}')
          
          if not is_prerelease and not is_latest:
              print(f'Adding stable release {version_tag} to benchmark history...')
              
              # Load existing history
              history_file = 'docs/benchmark-history.json'
              try:
                  with open(history_file, 'r') as f:
                      history = json.load(f)
              except (FileNotFoundError, json.JSONDecodeError):
                  history = []
              
              # Check if this version already exists (update it) or add new
              version_exists = False
              for i, entry in enumerate(history):
                  if entry.get('version') == new_result['version']:
                      history[i] = new_result
                      version_exists = True
                      break
              
              if not version_exists:
                  history.append(new_result)
              
              # Keep only last 50 entries
              history = history[-50:]
              
              # Save updated history
              with open(history_file, 'w') as f:
                  json.dump(history, f, indent=2)
              
              print(f'History updated: {len(history)} stable releases')
          else:
              print(f'Skipping history update for pre-release/latest version: {version_tag}')
          "

      - name: Commit and push pricing updates
        if: success()
        run: |
          cd $GITHUB_WORKSPACE
          pwd
          ls -la docs/
          git status
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          # Check if there are any changes to commit
          if git diff --quiet docs/index.html docs/benchmark-results.json docs/benchmark-history.json 2>/dev/null; then
            echo "No pricing changes to commit"
          else
            git add docs/index.html docs/benchmark-results.json docs/benchmark-history.json
            git commit -m "Update pricing and benchmark results [skip ci]"
            
            # Retry push with pull --rebase in case of concurrent updates
            MAX_RETRIES=3
            RETRY=0
            while [ $RETRY -lt $MAX_RETRIES ]; do
              if git push; then
                echo "âœ… Pricing and benchmark results updated and pushed"
                break
              else
                RETRY=$((RETRY+1))
                echo "Push failed, attempt $RETRY of $MAX_RETRIES. Pulling latest changes..."
                git pull --rebase origin main
              fi
            done
            
            if [ $RETRY -eq $MAX_RETRIES ]; then
              echo "âŒ Failed to push after $MAX_RETRIES attempts"
              exit 1
            fi
          fi

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: bench_results.json

      - name: Add summary
        run: |
          echo "## ðŸš€ Deployment and Benchmark Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Image:** \`${{ steps.vars.outputs.FULL_IMAGE }}\`" >> $GITHUB_STEP_SUMMARY
          echo "- **Model:** \`${{ env.MODEL }}\`" >> $GITHUB_STEP_SUMMARY
          echo "- **Container:** \`${{ env.CONTAINER_NAME }}\` running on port ${{ env.VLLM_PORT }}" >> $GITHUB_STEP_SUMMARY
          echo "- **DGX Spark Cost:** \`\$${{ env.DGX_COST_PER_HOUR }}/hour\`" >> $GITHUB_STEP_SUMMARY
          echo "- **LMCache:** Enabled (CPU offload)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Benchmark Results (vllm bench serve)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Prefill (Input) | Cached (Input) | Decode (Output) |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-----------------|----------------|-----------------|" >> $GITHUB_STEP_SUMMARY
          echo "| Throughput | ${{ steps.benchmark.outputs.prefill_tps }} tok/s | ${{ steps.benchmark.outputs.cached_tps }} tok/s | ${{ steps.benchmark.outputs.decode_tps }} tok/s |" >> $GITHUB_STEP_SUMMARY
          echo "| Cost per 1M tokens | \$${{ steps.benchmark.outputs.cost_in }} | \$${{ steps.benchmark.outputs.cost_cached }} | \$${{ steps.benchmark.outputs.cost_out }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f bench_results.json ]; then
            echo "### Raw Results" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`json" >> $GITHUB_STEP_SUMMARY
            cat bench_results.json >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          fi
