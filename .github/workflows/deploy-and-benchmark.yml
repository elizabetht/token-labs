name: Deploy and Benchmark on DGX Spark

on:
  workflow_run:
    workflows: ["Build and push vLLM image"]
    types: [completed]
  workflow_dispatch:
    inputs:
      image_tag:
        description: 'Image tag to deploy (default: latest)'
        required: false
        default: 'latest'

permissions:
  contents: write
  packages: read

env:
  CONTAINER_NAME: vllm-server
  VLLM_PORT: 8000
  MODEL: meta-llama/Llama-3.1-8B-Instruct
  DRAFT_MODEL: meta-llama/Llama-3.2-1B-Instruct

  # DGX Spark economics: $4000 hardware / 26280 hours (3yr) = ~$0.152/hr base + ~$0.02/hr electricity
  # Base cost scales with GPU utilization, electricity is fixed overhead
  DGX_HARDWARE_COST_PER_HOUR: "0.152"  # Hardware amortization at 100% utilization
  DGX_ELECTRICITY_COST_PER_HOUR: "0.02"  # Fixed electricity cost
  GPU_MEMORY_UTILIZATION: "0.6"  # Must match --gpu-memory-utilization in docker run
  DGX_TAILSCALE_IP: "100.64.38.13"

jobs:
  deploy-and-benchmark:
    runs-on: [self-hosted, DGX-Spark]
    # Only run if build succeeded or manually triggered
    if: ${{ github.event.workflow_run.conclusion == 'success' || github.event_name == 'workflow_dispatch' }}
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          ref: main

      - name: Fix Git ownership
        run: git config --global --add safe.directory "$GITHUB_WORKSPACE"

      - name: Determine image tag
        id: vars
        run: |
          OWNER=$(echo "${GITHUB_REPOSITORY_OWNER}" | tr '[:upper:]' '[:lower:]')
          IMAGE="ghcr.io/${OWNER}/token-labs/vllm-serve"
          
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            TAG="${{ github.event.inputs.image_tag }}"
          else
            # Get version from the tag that triggered the build workflow
            # workflow_run event provides head_branch which contains the tag name
            TAG="${{ github.event.workflow_run.head_branch }}"
            # Fallback to latest if tag is empty
            if [ -z "$TAG" ]; then
              TAG="latest"
            fi
          fi
          
          # For version tracking, remove 'v' prefix if present (v0.1.0 -> 0.1.0)
          VERSION_TAG="${TAG#v}"
          
          echo "IMAGE=${IMAGE}" >> "$GITHUB_OUTPUT"
          echo "TAG=${TAG}" >> "$GITHUB_OUTPUT"
          echo "VERSION_TAG=${VERSION_TAG}" >> "$GITHUB_OUTPUT"
          echo "FULL_IMAGE=${IMAGE}:${TAG}" >> "$GITHUB_OUTPUT"
          echo "Using image tag: $TAG (version: $VERSION_TAG)"

      - name: Log in to GHCR
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Pull latest image
        run: |
          echo "Pulling image: ${{ steps.vars.outputs.FULL_IMAGE }}"
          # Try to pull the image with the specified tag first
          if ! docker pull ${{ steps.vars.outputs.FULL_IMAGE }}; then
            # If that fails and the tag starts with 'v', try without the 'v' prefix
            TAG="${{ steps.vars.outputs.TAG }}"
            if [[ "${TAG}" == v* ]]; then
              FALLBACK_TAG="${TAG#v}"
              FALLBACK_IMAGE="${{ steps.vars.outputs.IMAGE }}:${FALLBACK_TAG}"
              echo "Failed to pull ${{ steps.vars.outputs.FULL_IMAGE }}, trying fallback: ${FALLBACK_IMAGE}"
              docker pull "${FALLBACK_IMAGE}"
              # Update the image reference for subsequent steps
              echo "FULL_IMAGE=${FALLBACK_IMAGE}" >> $GITHUB_ENV
            else
              echo "Failed to pull image and no fallback available"
              exit 1
            fi
          fi

      - name: Stop existing vLLM container (if running)
        run: |
          if docker ps -q -f name=${{ env.CONTAINER_NAME }} | grep -q .; then
            echo "Stopping existing container..."
            docker stop ${{ env.CONTAINER_NAME }}
          fi
          if docker ps -aq -f name=${{ env.CONTAINER_NAME }} | grep -q .; then
            echo "Removing existing container..."
            docker rm ${{ env.CONTAINER_NAME }}
          fi

      - name: Start vLLM server
        run: |
          echo "Starting vLLM server with model: ${{ env.MODEL }} (Speculative Decoding enabled)"
          CONTAINER_IMAGE="${FULL_IMAGE:-${{ steps.vars.outputs.FULL_IMAGE }}}"
          echo "Using container image: ${CONTAINER_IMAGE}"
          docker run -d \
            --gpus all \
            --name ${{ env.CONTAINER_NAME }} \
            -p ${{ env.DGX_TAILSCALE_IP}}:${{ env.VLLM_PORT }}:8000 \
            -e HF_TOKEN=${{ secrets.HF_TOKEN }} \
            -e FLASHINFER_DISABLE_VERSION_CHECK=1 \
            -v $HOME/.cache/huggingface:/root/.cache/huggingface \
            -v ${{ github.workspace }}/config:/app/config:ro \
            "${CONTAINER_IMAGE}" \
            ${{ env.MODEL }} \
            --gpu-memory-utilization ${{ env.GPU_MEMORY_UTILIZATION }} \
            --speculative-config '{"model": "RedHatAI/Llama-3.1-8B-Instruct-speculator.eagle3", "num_speculative_tokens": 7, "method": "eagle3"}'

      - name: Wait for vLLM server to be ready
        run: |
          echo "Waiting for vLLM server to be ready..."
          MAX_ATTEMPTS=300
          ATTEMPT=0
          while [ $ATTEMPT -lt $MAX_ATTEMPTS ]; do
            if curl -s http://${{ env.DGX_TAILSCALE_IP}}:${{ env.VLLM_PORT }}/health > /dev/null 2>&1; then
              echo "âœ… vLLM server is ready!"
              break
            fi
            ATTEMPT=$((ATTEMPT+1))
            sleep 1
          done
          if [ $ATTEMPT -eq $MAX_ATTEMPTS ]; then
            echo "âŒ Server failed to start within timeout"
            docker logs ${{ env.CONTAINER_NAME }} --tail 100
            exit 1
          fi

      - name: Run vLLM serving benchmark
        id: benchmark
        run: |
          # Run separate benchmarks for prefill-heavy and decode-heavy workloads
          echo "Running vLLM serving benchmarks..."
          
          # Calculate dynamic cost based on GPU utilization
          # Total cost = (Hardware cost Ã— GPU utilization) + Fixed electricity cost
          GPU_UTIL=${{ env.GPU_MEMORY_UTILIZATION }}
          HARDWARE_COST=${{ env.DGX_HARDWARE_COST_PER_HOUR }}
          ELECTRICITY_COST=${{ env.DGX_ELECTRICITY_COST_PER_HOUR }}
          
          DGX_COST=$(echo "scale=4; ($HARDWARE_COST * $GPU_UTIL) + $ELECTRICITY_COST" | bc)
          echo "GPU Memory Utilization: ${GPU_UTIL} (${GPU_UTIL}%)"
          echo "Hardware cost (scaled): \$$(echo "scale=4; $HARDWARE_COST * $GPU_UTIL" | bc)/hr"
          echo "Electricity cost (fixed): \$${ELECTRICITY_COST}/hr"
          echo "Total DGX cost per hour: \$${DGX_COST}/hr"
          
          echo "========================================"
          echo "Running PREFIX CACHE benchmark first to establish cache..."
          echo "========================================"
          
          # === RUN PREFIX CACHE BENCHMARK FIRST ===
          echo "Starting PREFIX CACHE benchmark to populate cache and get server metrics..."
          docker exec ${{ env.CONTAINER_NAME }} \
            bash -c "source /opt/venv/bin/activate && \
            FLASHINFER_DISABLE_VERSION_CHECK=1 \
            vllm bench serve \
            --model ${{ env.MODEL }} \
            --base-url http://${{ env.DGX_TAILSCALE_IP }}:${{ env.VLLM_PORT }} \
            --dataset-name prefix_repetition \
            --num-prompts 100 \
            --prefix-repetition-prefix-len 512 \
            --prefix-repetition-suffix-len 128 \
            --prefix-repetition-num-prefixes 5 \
            --prefix-repetition-output-len 128" \
            2>&1 | tee cache_bench.txt
          CACHE_EXIT=$?
          
          if [ $CACHE_EXIT -ne 0 ]; then
            echo "âŒ PREFIX CACHE benchmark failed (exit code: $CACHE_EXIT)"
            echo "Cache benchmark output:"
            cat cache_bench.txt
            exit 1
          fi
          
          echo "âœ… PREFIX CACHE benchmark completed successfully!"
          
          # === EXTRACT SERVER METRICS IMMEDIATELY AFTER CACHE BENCHMARK ===
          echo ""
          echo "========================================"
          echo "EXTRACTING SERVER METRICS FOR CACHED THROUGHPUT CALCULATION"
          echo "(Extracting while cache is populated and metrics are fresh)"
          echo "========================================"
          
          # Get recent server logs to extract vLLM server metrics
          # Use --since to only get logs from this container's current run
          CONTAINER_START_TIME=$(docker inspect --format='{{.State.StartedAt}}' ${{ env.CONTAINER_NAME }})
          docker logs ${{ env.CONTAINER_NAME }} --since "$CONTAINER_START_TIME" > server_logs.txt
          
          # Extract avg prompt throughput from vLLM server logs (filter for non-zero values)
          # Pattern: "Engine 000: Avg prompt throughput: 921.5 tokens/s" (exclude 0.0 values)
          SERVER_PROMPT_TPS=$(grep -oP 'Avg prompt throughput: \K[\d.]+(?= tokens/s)' server_logs.txt | \
                              grep -v '^0\.0*$' | tail -1 || echo "0")
          
          # Extract cache hit rate from vLLM server logs
          # With LMCache disabled, look for internal "Prefix cache hit rate" (vLLM automatic caching)
          # Pattern: "Prefix cache hit rate: 10.9%" (NOT "External prefix cache hit rate")
          SERVER_CACHE_HIT_RATE=$(grep -oP '(?<!External )Prefix cache hit rate: \K[\d.]+(?=%)' server_logs.txt | tail -1 || echo "0")
          
          echo "Extracted server metrics (non-zero prompt throughput only):"
          echo "- Raw prompt throughput values found: $(grep -oP 'Avg prompt throughput: \K[\d.]+(?= tokens/s)' server_logs.txt | tr '\n' ' ')"
          echo "- Non-zero prompt throughput selected: $SERVER_PROMPT_TPS tokens/s"
          echo "- Prefix cache hit rate (internal): $SERVER_CACHE_HIT_RATE%"
          
          echo ""
          echo "========================================"
          echo "Now running PREFILL and DECODE benchmarks in parallel..."
          echo "========================================"
          
          # === RUN PREFILL AND DECODE BENCHMARKS IN PARALLEL ===
          
          # PREFILL-HEAVY BENCHMARK (3:1 input:output ratio)
          echo "Starting PREFILL-HEAVY benchmark in background..."
          docker exec ${{ env.CONTAINER_NAME }} \
            bash -c "source /opt/venv/bin/activate && \
            FLASHINFER_DISABLE_VERSION_CHECK=1 \
            vllm bench serve \
            --model ${{ env.MODEL }} \
            --base-url http://${{ env.DGX_TAILSCALE_IP }}:${{ env.VLLM_PORT }} \
            --num-prompts 100 \
            --request-rate 10 \
            --random-input-len 3072 \
            --random-output-len 1024" \
            > prefill_bench.txt 2>&1 &
          PREFILL_PID=$!
          
          # DECODE-HEAVY BENCHMARK (1:3 input:output ratio)  
          echo "Starting DECODE-HEAVY benchmark in background..."
          docker exec ${{ env.CONTAINER_NAME }} \
            bash -c "source /opt/venv/bin/activate && \
            FLASHINFER_DISABLE_VERSION_CHECK=1 \
            vllm bench serve \
            --model ${{ env.MODEL }} \
            --base-url http://${{ env.DGX_TAILSCALE_IP }}:${{ env.VLLM_PORT }} \
            --num-prompts 100 \
            --request-rate 10 \
            --random-input-len 1024 \
            --random-output-len 3072" \
            > decode_bench.txt 2>&1 &
          DECODE_PID=$!
          
          echo "Waiting for prefill and decode benchmarks to complete..."
          
          # Wait for parallel benchmarks to complete
          wait $PREFILL_PID
          PREFILL_EXIT=$?
          echo "âœ… Prefill benchmark completed (exit code: $PREFILL_EXIT)"
          
          wait $DECODE_PID  
          DECODE_EXIT=$?
          echo "âœ… Decode benchmark completed (exit code: $DECODE_EXIT)"
          
          # Check if either benchmark failed
          if [ $PREFILL_EXIT -ne 0 ] || [ $DECODE_EXIT -ne 0 ]; then
            echo "âŒ One or more benchmarks failed"
            echo "Prefill exit code: $PREFILL_EXIT"
            echo "Decode exit code: $DECODE_EXIT"
            
            # Show benchmark outputs for debugging
            if [ $PREFILL_EXIT -ne 0 ]; then
              echo ""
              echo "=== PREFILL BENCHMARK ERROR OUTPUT ==="
              cat prefill_bench.txt
            fi
            
            if [ $DECODE_EXIT -ne 0 ]; then
              echo ""
              echo "=== DECODE BENCHMARK ERROR OUTPUT ==="
              cat decode_bench.txt
            fi
            
            exit 1
          fi
          
          echo "========================================"
          echo "All benchmarks completed successfully! Processing results..."
          echo "========================================"
          
          # Display results
          echo ""
          echo "=== PREFILL-HEAVY BENCHMARK RESULTS ==="
          cat prefill_bench.txt
          
          echo ""
          echo "=== DECODE-HEAVY BENCHMARK RESULTS ==="  
          cat decode_bench.txt
          
          echo ""
          echo "=== PREFIX CACHE BENCHMARK RESULTS ==="
          cat cache_bench.txt
          
          echo ""
          echo "========================================"
          echo "EXTRACTING METRICS FROM ALL BENCHMARKS"
          echo "========================================"
          
          # Extract prefill throughput (input tokens/s) from prefill-heavy benchmark
          # Use Total token throughput as the primary metric for prefill-heavy workload
          PREFILL_TPS=$(grep -oP 'Total token throughput \(tok/s\):\s*\K[\d.]+' prefill_bench.txt | tail -1 || echo "0")
          if [ "$PREFILL_TPS" = "0" ] || [ -z "$PREFILL_TPS" ]; then
            # Fallback to output token throughput if total not found
            PREFILL_TPS=$(grep -oP 'Output token throughput \(tok/s\):\s*\K[\d.]+' prefill_bench.txt | tail -1 || echo "0")
          fi
          echo "Prefill throughput: $PREFILL_TPS tokens/s"
          
          # Extract decode throughput (output tokens/s) from decode-heavy benchmark  
          DECODE_TPS=$(grep -oP 'Output token throughput \(tok/s\):\s*\K[\d.]+' decode_bench.txt | tail -1 || echo "0")
          if [ "$DECODE_TPS" = "0" ] || [ -z "$DECODE_TPS" ]; then
            # Fallback to total throughput if output not found
            DECODE_TPS=$(grep -oP 'Total token throughput \(tok/s\):\s*\K[\d.]+' decode_bench.txt | tail -1 || echo "0")
          fi
          echo "Decode throughput: $DECODE_TPS tokens/s"
          
          # Extract cached throughput from prefix_repetition benchmark
          # Use Total token throughput as the primary metric
          CACHE_TEST_TPS=$(grep -oP 'Total token throughput \(tok/s\):\s*\K[\d.]+' cache_bench.txt | tail -1 || echo "0")
          if [ "$CACHE_TEST_TPS" = "0" ] || [ -z "$CACHE_TEST_TPS" ]; then
            # Fallback to output token throughput
            CACHE_TEST_TPS=$(grep -oP 'Output token throughput \(tok/s\):\s*\K[\d.]+' cache_bench.txt | tail -1 || echo "0")
          fi
          echo "Cache test throughput: $CACHE_TEST_TPS tokens/s"
          
          # Extract prefix cache hit rate if available from server logs
          CACHE_HIT_RATE="$SERVER_CACHE_HIT_RATE"
          echo "Prefix cache hit rate: $CACHE_HIT_RATE%"
          
          # Calculate cached input throughput using server metrics formula:
          # Cached input throughput = Avg prompt throughput Ã— Prefix cache hit rate
          echo "========================================"
          echo ""
          echo "=== CALCULATING CACHED THROUGHPUT FROM EXTRACTED SERVER METRICS ==="
          
          # Use the server metrics extracted after cache benchmark
          echo "Using previously extracted server metrics:"
          echo "- Server prompt throughput: $SERVER_PROMPT_TPS tokens/s"
          echo "- Server cache hit rate: $SERVER_CACHE_HIT_RATE%"
          
          # Fallback to benchmark values if server metrics not found
          if [ "$SERVER_PROMPT_TPS" = "0" ] || [ -z "$SERVER_PROMPT_TPS" ]; then
            SERVER_PROMPT_TPS=$PREFILL_TPS
            echo "Fallback: Using benchmark prompt throughput: $SERVER_PROMPT_TPS tokens/s"
          fi
          
          if [ "$SERVER_CACHE_HIT_RATE" = "0" ] || [ -z "$SERVER_CACHE_HIT_RATE" ]; then
            SERVER_CACHE_HIT_RATE="$CACHE_HIT_RATE"
            echo "Fallback: Using benchmark cache hit rate: $SERVER_CACHE_HIT_RATE%"
          fi
          
          # Calculate cached input throughput using the formula:
          # cached_input_throughput = prompt_throughput Ã— (cache_hit_rate / 100)
          if [ "$SERVER_PROMPT_TPS" != "0" ] && [ "$SERVER_CACHE_HIT_RATE" != "0" ]; then
            CALCULATED_CACHED_TPS=$(echo "scale=2; $SERVER_PROMPT_TPS * ($SERVER_CACHE_HIT_RATE / 100)" | bc)
            echo "Calculated cached input throughput: $CALCULATED_CACHED_TPS tokens/s"
            echo "(Formula: $SERVER_PROMPT_TPS Ã— $SERVER_CACHE_HIT_RATE% = $CALCULATED_CACHED_TPS tokens/s)"
          else
            CALCULATED_CACHED_TPS="0"
            echo "Cannot calculate cached throughput: insufficient server metrics"
          fi
          
          # Use the better of benchmark result or calculated result
          if [ "$CACHE_TEST_TPS" != "0" ] && [ -n "$CACHE_TEST_TPS" ]; then
            CACHED_TPS=$CACHE_TEST_TPS
            echo "Using benchmark cached throughput: $CACHED_TPS tokens/s"
          elif [ "$CALCULATED_CACHED_TPS" != "0" ] && [ -n "$CALCULATED_CACHED_TPS" ]; then
            CACHED_TPS=$CALCULATED_CACHED_TPS
            echo "Using calculated cached throughput: $CACHED_TPS tokens/s"
          else
            CACHED_TPS="0"
            echo "No cached throughput available"
          fi
          
          echo "========================================"
          echo ""
          echo "=== CACHED THROUGHPUT CALCULATION COMPLETE ==="
          echo "Cached input throughput: $CACHED_TPS tokens/s"
          
          # Use extracted values
          INPUT_TPS=$PREFILL_TPS
          OUTPUT_TPS=$DECODE_TPS
          
          echo "========================================"
          echo ""
          echo "=== FINAL RESULTS ==="
          echo "Input (prefill) throughput: $INPUT_TPS tokens/s"
          echo "Cached input throughput: $CACHED_TPS tokens/s"
          echo "Output (decode) throughput: $OUTPUT_TPS tokens/s"
          
          # Calculate cost per 1M tokens: (cost_per_hour / tokens_per_sec) * 1_000_000 / 3600
          
          if [ "$INPUT_TPS" != "0" ] && [ -n "$INPUT_TPS" ]; then
            # Use awk to ensure proper decimal formatting (bc outputs .xxx instead of 0.xxx)
            COST_IN=$(echo "scale=4; $DGX_COST * 1000000 / ($INPUT_TPS * 3600)" | bc | awk '{printf "%.4f", $0}')
          else
            COST_IN="0"
          fi
          
          if [ "$OUTPUT_TPS" != "0" ] && [ -n "$OUTPUT_TPS" ]; then
            COST_OUT=$(echo "scale=4; $DGX_COST * 1000000 / ($OUTPUT_TPS * 3600)" | bc | awk '{printf "%.4f", $0}')
          else
            COST_OUT="0"
          fi
          
          # Calculate cost for cached tokens (should be cheaper)
          if [ "$CACHED_TPS" != "0" ] && [ -n "$CACHED_TPS" ]; then
            COST_CACHED=$(echo "scale=4; $DGX_COST * 1000000 / ($CACHED_TPS * 3600)" | bc | awk '{printf "%.4f", $0}')
          else
            COST_CACHED="0"
          fi
          
          echo "Cost per 1M input tokens: \$$COST_IN"
          echo "Cost per 1M cached tokens: \$$COST_CACHED"
          echo "Cost per 1M output tokens: \$$COST_OUT"
          
          # Ensure variables have valid numeric values for JSON
          # Default to 0 if empty or unset
          INPUT_TPS="${INPUT_TPS:-0}"
          OUTPUT_TPS="${OUTPUT_TPS:-0}"
          CACHED_TPS="${CACHED_TPS:-0}"
          COST_IN="${COST_IN:-0}"
          COST_OUT="${COST_OUT:-0}"
          COST_CACHED="${COST_CACHED:-0}"
          
          # Handle empty strings explicitly
          [[ -z "$INPUT_TPS" || "$INPUT_TPS" == "" ]] && INPUT_TPS="0"
          [[ -z "$OUTPUT_TPS" || "$OUTPUT_TPS" == "" ]] && OUTPUT_TPS="0"
          [[ -z "$CACHED_TPS" || "$CACHED_TPS" == "" ]] && CACHED_TPS="0"
          [[ -z "$COST_IN" || "$COST_IN" == "" ]] && COST_IN="0"
          [[ -z "$COST_OUT" || "$COST_OUT" == "" ]] && COST_OUT="0"
          [[ -z "$COST_CACHED" || "$COST_CACHED" == "" ]] && COST_CACHED="0"
          
          # Debug: print values before JSON generation
          echo "DEBUG: INPUT_TPS='$INPUT_TPS' OUTPUT_TPS='$OUTPUT_TPS' CACHED_TPS='$CACHED_TPS' COST_IN='$COST_IN' COST_OUT='$COST_OUT' COST_CACHED='$COST_CACHED'"
          
          # Set environment variables for the Python script
          export MODEL="${{ env.MODEL }}"
          export DGX_COST="${DGX_COST}"
          export FULL_IMAGE="${{ steps.vars.outputs.FULL_IMAGE }}"
          export GPU_MEMORY_UTILIZATION="${{ env.GPU_MEMORY_UTILIZATION }}"
          export SPECULATIVE_DECODING_ENABLED="true"
          export INPUT_TPS="${INPUT_TPS}"
          export OUTPUT_TPS="${OUTPUT_TPS}"
          export CACHED_TPS="${CACHED_TPS}"
          export COST_IN="${COST_IN}"
          export COST_OUT="${COST_OUT}"
          export COST_CACHED="${COST_CACHED}"
          
          # Benchmark parameters for dynamic HTML generation
          export PREFILL_NUM_PROMPTS="100"
          export PREFILL_REQUEST_RATE="10"
          export PREFILL_INPUT_LEN="3072"
          export PREFILL_OUTPUT_LEN="1024"
          export DECODE_NUM_PROMPTS="100"
          export DECODE_REQUEST_RATE="10"
          export DECODE_INPUT_LEN="1024"
          export DECODE_OUTPUT_LEN="3072"
          export CACHE_NUM_PROMPTS="100"
          export CACHE_PREFIX_LEN="512"
          export CACHE_SUFFIX_LEN="128"
          export CACHE_NUM_PREFIXES="5"
          export CACHE_OUTPUT_LEN="128"
          
          # Generate results JSON using Python script
          python3 scripts/generate_results.py
          
          # Set outputs for summary
          echo "prefill_tps=$INPUT_TPS" >> $GITHUB_OUTPUT
          echo "cached_tps=$CACHED_TPS" >> $GITHUB_OUTPUT
          echo "decode_tps=$OUTPUT_TPS" >> $GITHUB_OUTPUT
          echo "cost_in=$COST_IN" >> $GITHUB_OUTPUT
          echo "cost_cached=$COST_CACHED" >> $GITHUB_OUTPUT
          echo "cost_out=$COST_OUT" >> $GITHUB_OUTPUT

      - name: Update pricing in docs
        if: success()
        run: |
          python3 scripts/update_pricing.py \
            --results bench_results.json \
            --html docs/index.html
          
          # Also copy results to docs for GitHub Pages
          cp bench_results.json docs/benchmark-results.json
          
          # Append to benchmark history
          python3 -c "
          import json
          import re
          from datetime import datetime
          
          # Load new results
          with open('bench_results.json', 'r') as f:
              new_result = json.load(f)
          
          # Add version from image tag
          version_tag = '${{ steps.vars.outputs.VERSION_TAG }}'
          new_result['version'] = version_tag
          
          # Only add to history for stable releases (not pre-releases)
          # Skip versions with: rc, beta, alpha, dev, pre, snapshot
          is_prerelease = bool(re.search(r'\b(rc|beta|alpha|dev|pre|snapshot)\b', version_tag, re.IGNORECASE))
          is_latest = version_tag == 'latest'
          
          print(f'Version: {version_tag}, Is pre-release: {is_prerelease}, Is latest: {is_latest}')
          
          if not is_prerelease and not is_latest:
              print(f'Adding stable release {version_tag} to benchmark history...')
              
              # Load existing history
              history_file = 'docs/benchmark-history.json'
              try:
                  with open(history_file, 'r') as f:
                      history = json.load(f)
              except (FileNotFoundError, json.JSONDecodeError):
                  history = []
              
              # Check if this version already exists (update it) or add new
              version_exists = False
              for i, entry in enumerate(history):
                  if entry.get('version') == new_result['version']:
                      history[i] = new_result
                      version_exists = True
                      break
              
              if not version_exists:
                  history.append(new_result)
              
              # Keep only last 50 entries
              history = history[-50:]
              
              # Save updated history
              with open(history_file, 'w') as f:
                  json.dump(history, f, indent=2)
              
              print(f'History updated: {len(history)} stable releases')
          else:
              print(f'Skipping history update for pre-release/latest version: {version_tag}')
          "

      - name: Commit and push pricing updates
        if: success()
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          # Check if there are any changes to commit
          if git diff --quiet docs/index.html docs/benchmark-results.json docs/benchmark-history.json 2>/dev/null; then
            echo "No pricing changes to commit"
          else
            git add docs/index.html docs/benchmark-results.json docs/benchmark-history.json
            git commit -m "Update pricing and benchmark results [skip ci]"
            
            # Retry push with pull --rebase in case of concurrent updates
            MAX_RETRIES=3
            RETRY=0
            while [ $RETRY -lt $MAX_RETRIES ]; do
              if git push origin main; then
                echo "âœ… Pricing and benchmark results updated and pushed"
                break
              else
                RETRY=$((RETRY+1))
                echo "Push failed, attempt $RETRY of $MAX_RETRIES. Pulling latest changes..."
                git pull --rebase origin main
              fi
            done
            
            if [ $RETRY -eq $MAX_RETRIES ]; then
              echo "âŒ Failed to push after $MAX_RETRIES attempts"
              exit 1
            fi
          fi

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: bench_results.json

      - name: Add summary
        run: |
          # Calculate cost for display (same formula as benchmark step)
          GPU_UTIL=${{ env.GPU_MEMORY_UTILIZATION }}
          HARDWARE_COST=${{ env.DGX_HARDWARE_COST_PER_HOUR }}
          ELECTRICITY_COST=${{ env.DGX_ELECTRICITY_COST_PER_HOUR }}
          DGX_COST=$(echo "scale=4; ($HARDWARE_COST * $GPU_UTIL) + $ELECTRICITY_COST" | bc)
          
          echo "## ðŸš€ Deployment and Benchmark Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Image:** \`${{ steps.vars.outputs.FULL_IMAGE }}\`" >> $GITHUB_STEP_SUMMARY
          echo "- **Model:** \`${{ env.MODEL }}\`" >> $GITHUB_STEP_SUMMARY
          echo "- **Container:** \`${{ env.CONTAINER_NAME }}\` running on port ${{ env.VLLM_PORT }}" >> $GITHUB_STEP_SUMMARY
          echo "- **GPU Memory Utilization:** \`${{ env.GPU_MEMORY_UTILIZATION }}\` (60%)" >> $GITHUB_STEP_SUMMARY
          echo "- **DGX Spark Cost:** \`\$${DGX_COST}/hour\` (hardware: \$$(echo \"scale=4; $HARDWARE_COST * $GPU_UTIL\" | bc) + electricity: \$${ELECTRICITY_COST})" >> $GITHUB_STEP_SUMMARY
          echo "- **LMCache:** Enabled (CPU offload)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Benchmark Results (vllm bench serve)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Prefill (Input) | Cached Prefill (Input) | Decode (Output) |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-----------------|------------------------|-----------------|" >> $GITHUB_STEP_SUMMARY
          echo "| **Throughput** | ${{ steps.benchmark.outputs.prefill_tps }} tok/s | ${{ steps.benchmark.outputs.cached_tps }} tok/s | ${{ steps.benchmark.outputs.decode_tps }} tok/s |" >> $GITHUB_STEP_SUMMARY
          echo "| **Cost per 1M tokens** | \$${{ steps.benchmark.outputs.cost_in }} | \$${{ steps.benchmark.outputs.cost_cached }} | \$${{ steps.benchmark.outputs.cost_out }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f bench_results.json ]; then
            echo "### Raw Results" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`json" >> $GITHUB_STEP_SUMMARY
            cat bench_results.json >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          fi
