name: Deploy and Benchmark on DGX Spark

on:
  workflow_run:
    workflows: ["Build and push vLLM image"]
    types: [completed]
  workflow_dispatch:
    inputs:
      image_tag:
        description: 'Image tag to deploy (default: latest)'
        required: false
        default: 'latest'

permissions:
  contents: write
  packages: read

env:
  CONTAINER_NAME: vllm-server
  VLLM_PORT: 8000
  MODEL: meta-llama/Llama-3.1-8B-Instruct
  DRAFT_MODEL: RedHatAI/Llama-3.1-8B-Instruct-speculator.eagle3
  DGX_HARDWARE_COST_PER_HOUR: "0.152"
  DGX_ELECTRICITY_COST_PER_HOUR: "0.02"
  GPU_MEMORY_UTILIZATION: "0.6"
  DGX_TAILSCALE_IP: "100.64.38.13"

jobs:
  deploy-and-benchmark:
    runs-on: [self-hosted, DGX-Spark]
    if: ${{ github.event.workflow_run.conclusion == 'success' || github.event_name == 'workflow_dispatch' }}
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          ref: main

      - name: Determine image tag
        id: vars
        run: |
          OWNER=$(echo "${GITHUB_REPOSITORY_OWNER}" | tr '[:upper:]' '[:lower:]')
          IMAGE="ghcr.io/${OWNER}/token-labs/vllm-serve"
          TAG="${{ github.event.inputs.image_tag || github.event.workflow_run.head_branch || 'latest' }}"
          VERSION_TAG="${TAG#v}"
          
          echo "IMAGE=${IMAGE}" >> "$GITHUB_OUTPUT"
          echo "TAG=${TAG}" >> "$GITHUB_OUTPUT"
          echo "VERSION_TAG=${VERSION_TAG}" >> "$GITHUB_OUTPUT"
          echo "FULL_IMAGE=${IMAGE}:${TAG}" >> "$GITHUB_OUTPUT"

      - name: Login and pull image
        run: |
          echo "${{ secrets.GITHUB_TOKEN }}" | docker login ghcr.io -u ${{ github.actor }} --password-stdin
          docker pull ${{ steps.vars.outputs.FULL_IMAGE }} || docker pull ${{ steps.vars.outputs.IMAGE}}:${TAG#v}

      - name: Stop existing container
        run: |
          docker stop ${{ env.CONTAINER_NAME }} 2>/dev/null || true
          docker rm ${{ env.CONTAINER_NAME }} 2>/dev/null || true

      - name: Start vLLM server
        run: |
          VERSION="${{ steps.vars.outputs.VERSION_TAG }}"
          IMAGE="${FULL_IMAGE:-${{ steps.vars.outputs.FULL_IMAGE }}}"
          
          case "$VERSION" in
            0.1.0|v0.1.0)
              export LMCACHE_ENABLED="false" SPECULATIVE_DECODING_ENABLED="false" PREFIX_CACHING_ENABLED="false"
              docker run -d --gpus all --name ${{ env.CONTAINER_NAME }} \
                -p ${{ env.DGX_TAILSCALE_IP}}:${{ env.VLLM_PORT }}:8000 \
                -e HF_TOKEN=${{ secrets.HF_TOKEN }} \
                -e FLASHINFER_DISABLE_VERSION_CHECK=1 \
                -v $HOME/.cache/huggingface:/root/.cache/huggingface \
                "${IMAGE}" ${{ env.MODEL }} --gpu-memory-utilization 0.3
              ;;
            0.2.0|v0.2.0)
              # v0.2.0 has LMCache configuration baked into the Docker image
              # No environment variables or volume mounts needed for LMCache
              # All settings (chunk_size, local_cpu, max_local_cpu_size, kv-transfer-config)
              # are pre-configured in the Dockerfile
              docker run -d --gpus all --name ${{ env.CONTAINER_NAME }} \
                -p ${{ env.DGX_TAILSCALE_IP}}:${{ env.VLLM_PORT }}:8000 \
                -e HF_TOKEN=${{ secrets.HF_TOKEN }} \
                -e FLASHINFER_DISABLE_VERSION_CHECK=1 \
                -v $HOME/.cache/huggingface:/root/.cache/huggingface \
                "${IMAGE}" ${{ env.MODEL }}
              ;;
            *)
              export LMCACHE_ENABLED="false" SPECULATIVE_DECODING_ENABLED="true" PREFIX_CACHING_ENABLED="true"
              docker run -d --gpus all --name ${{ env.CONTAINER_NAME }} \
                -p ${{ env.DGX_TAILSCALE_IP}}:${{ env.VLLM_PORT }}:8000 \
                -e HF_TOKEN=${{ secrets.HF_TOKEN }} \
                -e FLASHINFER_DISABLE_VERSION_CHECK=1 \
                -v $HOME/.cache/huggingface:/root/.cache/huggingface \
                "${IMAGE}" ${{ env.MODEL }} \
                --gpu-memory-utilization ${{ env.GPU_MEMORY_UTILIZATION }} \
                --enable-prefix-caching \
                --speculative-config '{"model": "${{ env.DRAFT_MODEL }}", "num_speculative_tokens": 7, "method": "eagle3"}'
              ;;
          esac
          
          # Wait for server
          for i in {1..300}; do
            curl -sf http://${{ env.DGX_TAILSCALE_IP}}:${{ env.VLLM_PORT }}/health && break
            sleep 1
          done

      - name: Run benchmarks
        id: benchmark
        run: |
          VERSION="${{ steps.vars.outputs.VERSION_TAG }}"
          
          # Calculate cost
          GPU_UTIL=${{ env.GPU_MEMORY_UTILIZATION }}
          DGX_COST=$(echo "scale=4; (${{ env.DGX_HARDWARE_COST_PER_HOUR }} * $GPU_UTIL) + ${{ env.DGX_ELECTRICITY_COST_PER_HOUR }}" | bc)
          
          # Run benchmarks sequentially to avoid issues
          # Skip cache benchmark for v0.1.0 as it doesn't have prefix caching
          if [[ "$VERSION" != "0.1.0" && "$VERSION" != "v0.1.0" ]]; then
            echo "Running cache benchmark..."
            docker exec ${{ env.CONTAINER_NAME }} bash -c "source /opt/venv/bin/activate && \
              FLASHINFER_DISABLE_VERSION_CHECK=1 vllm bench serve \
              --model ${{ env.MODEL }} --base-url http://${{ env.DGX_TAILSCALE_IP }}:${{ env.VLLM_PORT }} \
              --dataset-name prefix_repetition --num-prompts 100 \
              --prefix-repetition-prefix-len 512 --prefix-repetition-suffix-len 128 \
              --prefix-repetition-num-prefixes 5 --prefix-repetition-output-len 128" \
              2>&1 | tee cache_bench.txt
          else
            echo "Skipping cache benchmark for v0.1.0 (no prefix caching support)"
            echo "" > cache_bench.txt
          fi
          
          echo "Running prefill benchmark..."
          docker exec ${{ env.CONTAINER_NAME }} bash -c "source /opt/venv/bin/activate && \
            FLASHINFER_DISABLE_VERSION_CHECK=1 vllm bench serve \
            --model ${{ env.MODEL }} --base-url http://${{ env.DGX_TAILSCALE_IP }}:${{ env.VLLM_PORT }} \
            --num-prompts 100 --request-rate 10 \
            --random-input-len 3072 --random-output-len 1024" \
            2>&1 | tee prefill_bench.txt
          
          echo "Running decode benchmark..."
          docker exec ${{ env.CONTAINER_NAME }} bash -c "source /opt/venv/bin/activate && \
            FLASHINFER_DISABLE_VERSION_CHECK=1 vllm bench serve \
            --model ${{ env.MODEL }} --base-url http://${{ env.DGX_TAILSCALE_IP }}:${{ env.VLLM_PORT }} \
            --num-prompts 100 --request-rate 10 \
            --random-input-len 1024 --random-output-len 3072" \
            2>&1 | tee decode_bench.txt
          
          # Extract metrics
          extract() { 
            local result=$(grep -oP "$2" "$1" 2>/dev/null | tail -1)
            echo "${result:-0}"
          }
          
          INPUT_TPS=$(extract prefill_bench.txt 'Total token throughput \(tok/s\):\s*\K[\d.]+')
          OUTPUT_TPS=$(extract decode_bench.txt 'Output token throughput \(tok/s\):\s*\K[\d.]+')
          CACHED_TPS=$(extract cache_bench.txt 'Total token throughput \(tok/s\):\s*\K[\d.]+')
          
          # Ensure we have valid numbers
          [[ "$INPUT_TPS" == "0" || -z "$INPUT_TPS" ]] && INPUT_TPS=$(extract prefill_bench.txt 'Output token throughput \(tok/s\):\s*\K[\d.]+')
          [[ "$OUTPUT_TPS" == "0" || -z "$OUTPUT_TPS" ]] && OUTPUT_TPS=$(extract decode_bench.txt 'Total token throughput \(tok/s\):\s*\K[\d.]+')
          [[ "$CACHED_TPS" == "0" || -z "$CACHED_TPS" ]] && CACHED_TPS=$(extract cache_bench.txt 'Output token throughput \(tok/s\):\s*\K[\d.]+')
          
          # Default to 0 if still empty
          INPUT_TPS="${INPUT_TPS:-0}"
          OUTPUT_TPS="${OUTPUT_TPS:-0}"
          CACHED_TPS="${CACHED_TPS:-0}"
          
          echo "Extracted throughput: INPUT=$INPUT_TPS OUTPUT=$OUTPUT_TPS CACHED=$CACHED_TPS"
          
          # Latency metrics
          for test in PREFILL DECODE CACHE; do
            file="${test,,}_bench.txt"
            for metric in MEAN_TTFT MEDIAN_TTFT P99_TTFT MEAN_TPOT MEDIAN_TPOT P99_TPOT MEAN_ITL MEDIAN_ITL P99_ITL; do
              pattern=$(echo $metric | sed 's/_/ /g' | sed 's/MEAN/Mean/;s/MEDIAN/Median/;s/P99/P99/;s/TTFT/TTFT/;s/TPOT/TPOT/;s/ITL/ITL/')
              val=$(extract "$file" "${pattern} \(ms\):\s*\K[\d.]+")
              val="${val:-0}"
              export "${test}_${metric}=$val"
            done
          done
          
          # Calculate costs (handle division by zero)
          calc_cost() { 
            if [ "$1" != "0" ] && [ -n "$1" ]; then
              echo "scale=4; $DGX_COST * 1000000 / ($1 * 3600)" | bc | awk '{printf "%.4f", $0}'
            else
              echo "0"
            fi
          }
          
          COST_IN=$(calc_cost $INPUT_TPS)
          COST_OUT=$(calc_cost $OUTPUT_TPS)
          COST_CACHED=$(calc_cost $CACHED_TPS)
          
          # Export for Python script
          export MODEL="${{ env.MODEL }}" 
          export DGX_COST 
          export FULL_IMAGE="${{ steps.vars.outputs.FULL_IMAGE }}"
          export GPU_MEMORY_UTILIZATION="${{ env.GPU_MEMORY_UTILIZATION }}"
          export INPUT_TPS OUTPUT_TPS CACHED_TPS COST_IN COST_OUT COST_CACHED
          export PREFILL_NUM_PROMPTS=100 PREFILL_REQUEST_RATE=10 PREFILL_INPUT_LEN=3072 PREFILL_OUTPUT_LEN=1024
          export DECODE_NUM_PROMPTS=100 DECODE_REQUEST_RATE=10 DECODE_INPUT_LEN=1024 DECODE_OUTPUT_LEN=3072
          export CACHE_NUM_PROMPTS=100 CACHE_PREFIX_LEN=512 CACHE_SUFFIX_LEN=128 CACHE_NUM_PREFIXES=5 CACHE_OUTPUT_LEN=128
          
          python3 scripts/generate_results.py
          
          # Set outputs
          echo "prefill_tps=$INPUT_TPS" >> $GITHUB_OUTPUT
          echo "decode_tps=$OUTPUT_TPS" >> $GITHUB_OUTPUT
          echo "cached_tps=$CACHED_TPS" >> $GITHUB_OUTPUT
          echo "cost_in=$COST_IN" >> $GITHUB_OUTPUT
          echo "cost_out=$COST_OUT" >> $GITHUB_OUTPUT
          echo "cost_cached=$COST_CACHED" >> $GITHUB_OUTPUT

      - name: Update docs
        if: success()
        run: |
          python3 scripts/update_pricing.py --results bench_results.json --html docs/index.html
          cp bench_results.json docs/benchmark-results.json
          
          # Update history
          python3 -c "
          import json, re
          with open('bench_results.json') as f: new_result = json.load(f)
          version = '${{ steps.vars.outputs.VERSION_TAG }}'
          new_result['version'] = version
          
          if not re.search(r'\b(rc|beta|alpha|dev|pre|snapshot|latest)\b', version, re.I):
              try:
                  with open('docs/benchmark-history.json') as f: history = json.load(f)
              except: history = []
              
              for i, e in enumerate(history):
                  if e.get('version') == version:
                      history[i] = new_result
                      break
              else:
                  history.append(new_result)
              
              with open('docs/benchmark-history.json', 'w') as f:
                  json.dump(history[-50:], f, indent=2)
          "

      - name: Commit and push
        if: success()
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add docs/*.{html,json} 2>/dev/null || true
          git diff --quiet --cached || {
            git commit -m "Update benchmark results [skip ci]"
            for i in {1..3}; do
              git push origin main && break || git pull --rebase origin main
            done
          }

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: bench_results.json

      - name: Summary
        run: |
          DGX_COST=$(echo "scale=4; (${{ env.DGX_HARDWARE_COST_PER_HOUR }} * ${{ env.GPU_MEMORY_UTILIZATION }}) + ${{ env.DGX_ELECTRICITY_COST_PER_HOUR }}" | bc)
          
          cat >> $GITHUB_STEP_SUMMARY << EOF
          ## ðŸš€ Benchmark Complete
          
          **Version:** \`${{ steps.vars.outputs.VERSION_TAG }}\`
          **Cost:** \`\$${DGX_COST}/hr\` (GPU: ${{ env.GPU_MEMORY_UTILIZATION }})
          
          | Metric | Prefill | Cached | Decode |
          |--------|---------|--------|--------|
          | **Throughput (tok/s)** | ${{ steps.benchmark.outputs.prefill_tps }} | ${{ steps.benchmark.outputs.cached_tps }} | ${{ steps.benchmark.outputs.decode_tps }} |
          | **Cost per 1M tokens** | \$${{ steps.benchmark.outputs.cost_in }} | \$${{ steps.benchmark.outputs.cost_cached }} | \$${{ steps.benchmark.outputs.cost_out }} |
          EOF
