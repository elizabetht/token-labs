name: Deploy and Benchmark on DGX Spark

on:
  workflow_run:
    workflows: ["Build and push vLLM image"]
    types: [completed]
  workflow_dispatch:
    inputs:
      image_tag:
        description: 'Image tag to deploy (default: latest)'
        required: false
        default: 'latest'
      model:
        description: 'Model to benchmark'
        required: false
        default: 'tokenlabsdotrun/Llama-3.1-8B-ModelOpt-NVFP4'
        type: choice
        options:
          - 'meta-llama/Llama-3.1-8B-Instruct'
          - 'tokenlabsdotrun/Llama-3.1-8B-ModelOpt-NVFP4'
          - 'tokenlabsdotrun/Llama-3.1-8B-ModelOpt-FP8'
          - 'tokenlabsdotrun/Llama-3.1-8B-Unsloth-FP8-QAT'
      ifeval_samples:
        description: 'Number of IFEval samples (50 for quick, 541 for full)'
        required: false
        default: '50'
        type: choice
        options:
          - '50'
          - '100'
          - '200'
          - '541'
      accuracy_only:
        description: 'Run only accuracy tests (skip performance benchmarks)'
        required: false
        default: false
        type: boolean

permissions:
  contents: write
  packages: read

env:
  CONTAINER_NAME: vllm-server
  VLLM_PORT: 8000
  MODEL: ${{ github.event.inputs.model || 'tokenlabsdotrun/Llama-3.1-8B-ModelOpt-NVFP4' }}
  DRAFT_MODEL: RedHatAI/Llama-3.1-8B-Instruct-speculator.eagle3
  DGX_HARDWARE_COST_PER_HOUR: "0.152"
  DGX_ELECTRICITY_COST_PER_HOUR: "0.02"
  GPU_MEMORY_UTILIZATION: "0.6"
  DGX_TAILSCALE_IP: "100.64.38.13"

jobs:
  deploy-and-benchmark:
    runs-on: [self-hosted, DGX-Spark]
    if: ${{ github.event.workflow_run.conclusion == 'success' || github.event_name == 'workflow_dispatch' }}
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          ref: main

      - name: Determine image tag
        id: vars
        run: |
          OWNER=$(echo "${GITHUB_REPOSITORY_OWNER}" | tr '[:upper:]' '[:lower:]')
          IMAGE="ghcr.io/${OWNER}/token-labs/vllm-serve"
          TAG="${{ github.event.inputs.image_tag || github.event.workflow_run.head_branch || 'latest' }}"
          VERSION_TAG="${TAG#v}"
          
          echo "IMAGE=${IMAGE}" >> "$GITHUB_OUTPUT"
          echo "TAG=${TAG}" >> "$GITHUB_OUTPUT"
          echo "VERSION_TAG=${VERSION_TAG}" >> "$GITHUB_OUTPUT"
          echo "FULL_IMAGE=${IMAGE}:${TAG}" >> "$GITHUB_OUTPUT"

      - name: Login and pull image
        run: |
          echo "${{ secrets.GITHUB_TOKEN }}" | docker login ghcr.io -u ${{ github.actor }} --password-stdin
          docker pull ${{ steps.vars.outputs.FULL_IMAGE }} || docker pull ${{ steps.vars.outputs.IMAGE}}:${TAG#v}

      - name: Stop existing container
        run: |
          docker stop ${{ env.CONTAINER_NAME }} 2>/dev/null || true
          docker rm ${{ env.CONTAINER_NAME }} 2>/dev/null || true

      - name: Start vLLM server
        run: |
          VERSION="${{ steps.vars.outputs.VERSION_TAG }}"
          IMAGE="${FULL_IMAGE:-${{ steps.vars.outputs.FULL_IMAGE }}}"
          
          case "$VERSION" in
            0.1.0|v0.1.0)
              export LMCACHE_ENABLED="false" SPECULATIVE_DECODING_ENABLED="false" PREFIX_CACHING_ENABLED="false"
              docker run -d --gpus all --name ${{ env.CONTAINER_NAME }} \
                -p ${{ env.DGX_TAILSCALE_IP}}:${{ env.VLLM_PORT }}:8000 \
                -e HF_TOKEN=${{ secrets.HF_TOKEN }} \
                -e FLASHINFER_DISABLE_VERSION_CHECK=1 \
                -v $HOME/.cache/huggingface:/root/.cache/huggingface \
                "${IMAGE}" ${{ env.MODEL }} --gpu-memory-utilization 0.3
              ;;
            0.2.0|v0.2.0)
              export LMCACHE_ENABLED="true" SPECULATIVE_DECODING_ENABLED="false" PREFIX_CACHING_ENABLED="false"
              docker run -d --gpus all --name ${{ env.CONTAINER_NAME }} \
                -p ${{ env.DGX_TAILSCALE_IP}}:${{ env.VLLM_PORT }}:8000 \
                -e HF_TOKEN=${{ secrets.HF_TOKEN }} \
                -e FLASHINFER_DISABLE_VERSION_CHECK=1 \
                -e LMCACHE_LOG_LEVEL=WARNING \
                -e LMCACHE_CONFIG_FILE=/app/config/lmcache-cpu-offload.yaml \
                -e LMCACHE_USE_EXPERIMENTAL=True \
                -v $HOME/.cache/huggingface:/root/.cache/huggingface \
                -v ${{ github.workspace }}/config:/app/config:ro \
                "${IMAGE}" ${{ env.MODEL }} \
                --gpu-memory-utilization 0.3 \
                --kv-transfer-config '{"kv_connector":"LMCacheConnectorV1","kv_role":"kv_both"}' \
                --no-enable-prefix-caching
              ;;
            *)
              export LMCACHE_ENABLED="false" SPECULATIVE_DECODING_ENABLED="true" PREFIX_CACHING_ENABLED="true"
              docker run -d --gpus all --name ${{ env.CONTAINER_NAME }} \
                -p ${{ env.DGX_TAILSCALE_IP}}:${{ env.VLLM_PORT }}:8000 \
                -e HF_TOKEN=${{ secrets.HF_TOKEN }} \
                -e FLASHINFER_DISABLE_VERSION_CHECK=1 \
                -v $HOME/.cache/huggingface:/root/.cache/huggingface \
                "${IMAGE}" ${{ env.MODEL }} \
                --gpu-memory-utilization ${{ env.GPU_MEMORY_UTILIZATION }} \
                --enable-prefix-caching \
                --speculative-config '{"model": "${{ env.DRAFT_MODEL }}", "num_speculative_tokens": 7, "method": "eagle3"}'
              ;;
          esac
          
          # Wait for server
          for i in {1..300}; do
            curl -sf http://${{ env.DGX_TAILSCALE_IP}}:${{ env.VLLM_PORT }}/health && break
            sleep 1
          done

      - name: Run IFEval accuracy evaluation
        id: ifeval
        run: |
          echo "Running IFEval accuracy benchmark with lighteval..."
          
          # Create virtual environment for evaluation
          python3 -m venv .venv-ifeval
          source .venv-ifeval/bin/activate
          
          # Install lighteval
          pip install --upgrade pip
          pip install 'lighteval[litellm]' langdetect
          
          # Run IFEval with a subset for CI (full eval takes ~30min)
          # Use 50 samples for quick CI, or set IFEVAL_SAMPLES for full eval
          NUM_SAMPLES="${{ github.event.inputs.ifeval_samples || '50' }}"
          
          # Run lighteval using litellm endpoint
          # Note: model_name needs openai/ prefix for litellm to use OpenAI-compatible API
          # api_key=dummy is required even though vLLM doesn't need authentication
          lighteval endpoint litellm \
            "model_name=openai/${{ env.MODEL }},base_url=http://${{ env.DGX_TAILSCALE_IP }}:${{ env.VLLM_PORT }}/v1,api_key=dummy" \
            ifeval \
            --output-dir ./lighteval_results \
            --max-samples "$NUM_SAMPLES" \
            --save-details || {
              echo "IFEval evaluation failed, continuing with zero accuracy"
              echo '{"prompt_level_accuracy": 0, "instruction_level_accuracy": 0, "num_samples": 0}' > ifeval_results.json
              echo "prompt_accuracy=0" >> $GITHUB_OUTPUT
              echo "instruction_accuracy=0" >> $GITHUB_OUTPUT
              echo "num_samples=0" >> $GITHUB_OUTPUT
              exit 0
            }
          
          # Find the latest results file
          RESULTS_FILE=$(find ./lighteval_results -name "results_*.json" | sort -r | head -1)
          
          if [ -z "$RESULTS_FILE" ]; then
            echo "No results file found, creating empty results"
            echo '{"prompt_level_accuracy": 0, "instruction_level_accuracy": 0, "num_samples": 0}' > ifeval_results.json
            echo "prompt_accuracy=0" >> $GITHUB_OUTPUT
            echo "instruction_accuracy=0" >> $GITHUB_OUTPUT
            echo "num_samples=0" >> $GITHUB_OUTPUT
          else
            # Parse lighteval results and convert to expected format
            cat > /tmp/parse_lighteval.py << 'PYEOF'
          import json
          import sys
          import os
          
          try:
              results_file = os.environ.get("RESULTS_FILE")
              if not results_file:
                  raise ValueError("RESULTS_FILE environment variable not set")
              
              with open(results_file) as f:
                  lighteval_results = json.load(f)
              
              # Extract IFEval metrics from lighteval results
              # lighteval returns results nested under task keys like "all" or "ifeval|0"
              results = lighteval_results.get("results", {})
              
              # Get metrics from "all" key (aggregated results)
              all_results = results.get("all", {})
              
              # Get strict accuracy metrics
              # Note: lighteval returns accuracy as a float between 0.0 and 1.0, so we multiply by 100 for percentage
              prompt_acc = all_results.get("prompt_level_strict_acc", 0) * 100
              inst_acc = all_results.get("inst_level_strict_acc", 0) * 100
              prompt_acc_loose = all_results.get("prompt_level_loose_acc", 0) * 100
              inst_acc_loose = all_results.get("inst_level_loose_acc", 0) * 100
              
              # Get actual number of samples from lighteval config
              config_general = lighteval_results.get("config_general", {})
              num_samples = config_general.get("max_samples") or int(os.environ.get("NUM_SAMPLES", "0"))
              
              # Create output in expected format
              output = {
                  "model": os.environ.get("MODEL", ""),
                  "num_samples": num_samples,
                  "prompt_level_accuracy": prompt_acc,
                  "instruction_level_accuracy": inst_acc,
                  "prompt_level_accuracy_loose": prompt_acc_loose,
                  "instruction_level_accuracy_loose": inst_acc_loose,
                  "timestamp": config_general.get("start_time", ""),
                  "raw_lighteval_results": results
              }
              
              print(json.dumps(output, indent=2))
          except Exception as e:
              error_output = {
                  "error": f"Failed to parse results: {str(e)}",
                  "prompt_level_accuracy": 0,
                  "instruction_level_accuracy": 0,
                  "num_samples": 0
              }
              print(json.dumps(error_output), file=sys.stderr)
              print('{"prompt_level_accuracy": 0, "instruction_level_accuracy": 0, "num_samples": 0}')
          PYEOF
            
            export RESULTS_FILE
            export MODEL="${{ env.MODEL }}"
            export NUM_SAMPLES
            python3 /tmp/parse_lighteval.py > ifeval_results.json
            
            # Extract results for GitHub outputs
            PROMPT_ACC=$(python3 -c "import json; print(json.load(open('ifeval_results.json')).get('prompt_level_accuracy', 0))")
            INST_ACC=$(python3 -c "import json; print(json.load(open('ifeval_results.json')).get('instruction_level_accuracy', 0))")
            NUM_SAMPLES=$(python3 -c "import json; print(json.load(open('ifeval_results.json')).get('num_samples', 0))")
            
            echo "prompt_accuracy=$PROMPT_ACC" >> $GITHUB_OUTPUT
            echo "instruction_accuracy=$INST_ACC" >> $GITHUB_OUTPUT
            echo "num_samples=$NUM_SAMPLES" >> $GITHUB_OUTPUT
            
            echo "IFEval Results: Prompt=${PROMPT_ACC}%, Instruction=${INST_ACC}%"
          fi

      - name: Compare with baseline
        id: baseline_comparison
        continue-on-error: true
        run: |
          # Determine baseline file based on model
          if [[ "${{ env.MODEL }}" == "meta-llama/Llama-3.1-8B-Instruct" ]]; then
            # For baseline model, update the baseline file
            echo "This is the baseline model. Updating baseline values..."
            if python3 scripts/compare_baseline.py \
              --results ifeval_results.json \
              --baseline baselines/llama-3.1-8b-instruct.json \
              --update-baseline \
              --run-id "${{ github.run_id }}"; then
              echo "comparison_status=BASELINE_UPDATED" >> $GITHUB_OUTPUT
            else
              echo "Failed to update baseline (exit code: $?)"
              echo "comparison_status=UPDATE_FAILED" >> $GITHUB_OUTPUT
            fi
          else
            # For other models, compare against baseline
            echo "Comparing against baseline model..."
            if python3 scripts/compare_baseline.py \
              --results ifeval_results.json \
              --baseline baselines/llama-3.1-8b-instruct.json \
              --output comparison_results.json; then
              # Comparison succeeded (model passed)
              if [ -f comparison_results.json ]; then
                COMPARISON_STATUS=$(python3 -c "import json; print(json.load(open('comparison_results.json')).get('status', 'UNKNOWN'))")
                echo "comparison_status=$COMPARISON_STATUS" >> $GITHUB_OUTPUT
                cat comparison_results.json
              fi
            else
              EXIT_CODE=$?
              # Exit code 1 means comparison failed (model degraded)
              # Other exit codes indicate script errors
              if [ $EXIT_CODE -eq 1 ]; then
                echo "Comparison completed: model accuracy degraded beyond threshold"
                if [ -f comparison_results.json ]; then
                  COMPARISON_STATUS=$(python3 -c "import json; print(json.load(open('comparison_results.json')).get('status', 'UNKNOWN'))")
                  echo "comparison_status=$COMPARISON_STATUS" >> $GITHUB_OUTPUT
                  cat comparison_results.json
                else
                  echo "comparison_status=FAILED" >> $GITHUB_OUTPUT
                fi
              else
                echo "Error running comparison script (exit code: $EXIT_CODE)"
                echo "comparison_status=ERROR" >> $GITHUB_OUTPUT
              fi
            fi
          fi

      - name: Run benchmarks
        id: benchmark
        if: ${{ github.event.inputs.accuracy_only != 'true' }}
        run: |
          VERSION="${{ steps.vars.outputs.VERSION_TAG }}"
          
          # Calculate cost
          GPU_UTIL=${{ env.GPU_MEMORY_UTILIZATION }}
          DGX_COST=$(echo "scale=4; (${{ env.DGX_HARDWARE_COST_PER_HOUR }} * $GPU_UTIL) + ${{ env.DGX_ELECTRICITY_COST_PER_HOUR }}" | bc)
          
          # Run benchmarks sequentially to avoid issues
          # Skip cache benchmark for v0.1.0 as it doesn't have prefix caching
          if [[ "$VERSION" != "0.1.0" && "$VERSION" != "v0.1.0" ]]; then
            echo "Running cache benchmark with guidellm..."
            docker exec ${{ env.CONTAINER_NAME }} bash -c "source /opt/venv/bin/activate && \
              FLASHINFER_DISABLE_VERSION_CHECK=1 guidellm benchmark \
              --target http://${{ env.DGX_TAILSCALE_IP }}:${{ env.VLLM_PORT }} \
              --profile constant \
              --rate 10 \
              --max-requests 100 \
              --data 'prompt_tokens=512,output_tokens=128' \
              --output-path /tmp/cache_bench" \
              2>&1 | tee cache_bench.txt
            
            # Copy benchmark results from container
            docker cp ${{ env.CONTAINER_NAME }}:/tmp/cache_bench/benchmarks.json cache_bench.json || echo '{}' > cache_bench.json
          else
            echo "Skipping cache benchmark for v0.1.0 (no prefix caching support)"
            echo '{}' > cache_bench.json
          fi
          
          echo "Running prefill benchmark with guidellm..."
          docker exec ${{ env.CONTAINER_NAME }} bash -c "source /opt/venv/bin/activate && \
            FLASHINFER_DISABLE_VERSION_CHECK=1 guidellm benchmark \
            --target http://${{ env.DGX_TAILSCALE_IP }}:${{ env.VLLM_PORT }} \
            --profile constant \
            --rate 10 \
            --max-requests 100 \
            --data 'prompt_tokens=3072,output_tokens=1024' \
            --output-path /tmp/prefill_bench" \
            2>&1 | tee prefill_bench.txt
          
          # Copy benchmark results from container
          docker cp ${{ env.CONTAINER_NAME }}:/tmp/prefill_bench/benchmarks.json prefill_bench.json || echo '{}' > prefill_bench.json
          
          echo "Running decode benchmark with guidellm..."
          docker exec ${{ env.CONTAINER_NAME }} bash -c "source /opt/venv/bin/activate && \
            FLASHINFER_DISABLE_VERSION_CHECK=1 guidellm benchmark \
            --target http://${{ env.DGX_TAILSCALE_IP }}:${{ env.VLLM_PORT }} \
            --profile constant \
            --rate 10 \
            --max-requests 100 \
            --data 'prompt_tokens=1024,output_tokens=3072' \
            --output-path /tmp/decode_bench" \
            2>&1 | tee decode_bench.txt
          
          # Copy benchmark results from container
          docker cp ${{ env.CONTAINER_NAME }}:/tmp/decode_bench/benchmarks.json decode_bench.json || echo '{}' > decode_bench.json
          
          # Parse guidellm JSON output and extract metrics
          cat > /tmp/parse_guidellm.py << 'PYEOF'
          import json
          import sys
          
          def extract_metrics(json_file):
              """Extract metrics from guidellm JSON output"""
              try:
                  with open(json_file) as f:
                      data = json.load(f)
                  
                  # guidellm outputs a list of benchmarks
                  if not data or 'benchmarks' not in data or not data['benchmarks']:
                      return {}
                  
                  # Get the first benchmark (we only run one per file)
                  benchmark = data['benchmarks'][0] if isinstance(data['benchmarks'], list) else data['benchmarks']
                  stats = benchmark.get('report', {})
                  
                  # Extract throughput metrics (tokens/second)
                  request_throughput = stats.get('request_throughput', 0)
                  prompt_token_throughput = stats.get('prompt_token_throughput', 0)
                  output_token_throughput = stats.get('output_token_throughput', 0)
                  total_token_throughput = stats.get('total_token_throughput', 0)
                  
                  # Extract latency metrics (convert to ms if needed)
                  ttft_mean = stats.get('ttft_mean', 0) * 1000  # Convert to ms
                  ttft_median = stats.get('ttft_median', 0) * 1000
                  ttft_p99 = stats.get('ttft_p99', 0) * 1000
                  
                  tpot_mean = stats.get('tpot_mean', 0) * 1000
                  tpot_median = stats.get('tpot_median', 0) * 1000
                  tpot_p99 = stats.get('tpot_p99', 0) * 1000
                  
                  itl_mean = stats.get('itl_mean', 0) * 1000
                  itl_median = stats.get('itl_median', 0) * 1000
                  itl_p99 = stats.get('itl_p99', 0) * 1000
                  
                  return {
                      'request_throughput': request_throughput,
                      'prompt_token_throughput': prompt_token_throughput,
                      'output_token_throughput': output_token_throughput,
                      'total_token_throughput': total_token_throughput,
                      'ttft_mean': ttft_mean,
                      'ttft_median': ttft_median,
                      'ttft_p99': ttft_p99,
                      'tpot_mean': tpot_mean,
                      'tpot_median': tpot_median,
                      'tpot_p99': tpot_p99,
                      'itl_mean': itl_mean,
                      'itl_median': itl_median,
                      'itl_p99': itl_p99
                  }
              except Exception as e:
                  print(f"Error parsing {json_file}: {e}", file=sys.stderr)
                  return {}
          
          # Parse each benchmark file
          prefill = extract_metrics('prefill_bench.json')
          decode = extract_metrics('decode_bench.json')
          cache = extract_metrics('cache_bench.json')
          
          # Output in shell variable format
          print(f"INPUT_TPS={prefill.get('prompt_token_throughput', 0)}")
          print(f"OUTPUT_TPS={decode.get('output_token_throughput', 0)}")
          print(f"CACHED_TPS={cache.get('total_token_throughput', 0)}")
          
          # Export latency metrics for prefill
          for key, val in prefill.items():
              if key.startswith(('ttft_', 'tpot_', 'itl_')):
                  metric_name = key.upper().replace('_', '_')
                  print(f"PREFILL_{metric_name.upper()}={val}")
          
          # Export latency metrics for decode
          for key, val in decode.items():
              if key.startswith(('ttft_', 'tpot_', 'itl_')):
                  metric_name = key.upper().replace('_', '_')
                  print(f"DECODE_{metric_name.upper()}={val}")
          
          # Export latency metrics for cache
          for key, val in cache.items():
              if key.startswith(('ttft_', 'tpot_', 'itl_')):
                  metric_name = key.upper().replace('_', '_')
                  print(f"CACHE_{metric_name.upper()}={val}")
          PYEOF
          
          # Run parser and source the output
          python3 /tmp/parse_guidellm.py > /tmp/metrics.env
          source /tmp/metrics.env
          
          # Default to 0 if still empty
          INPUT_TPS="${INPUT_TPS:-0}"
          OUTPUT_TPS="${OUTPUT_TPS:-0}"
          CACHED_TPS="${CACHED_TPS:-0}"
          
          echo "Extracted throughput: INPUT=$INPUT_TPS OUTPUT=$OUTPUT_TPS CACHED=$CACHED_TPS"
          
          # Calculate costs (handle division by zero)
          calc_cost() { 
            if [ "$1" != "0" ] && [ -n "$1" ]; then
              echo "scale=4; $DGX_COST * 1000000 / ($1 * 3600)" | bc | awk '{printf "%.4f", $0}'
            else
              echo "0"
            fi
          }
          
          COST_IN=$(calc_cost $INPUT_TPS)
          COST_OUT=$(calc_cost $OUTPUT_TPS)
          COST_CACHED=$(calc_cost $CACHED_TPS)
          
          # Export for Python script
          export MODEL="${{ env.MODEL }}" 
          export DGX_COST 
          export FULL_IMAGE="${{ steps.vars.outputs.FULL_IMAGE }}"
          export GPU_MEMORY_UTILIZATION="${{ env.GPU_MEMORY_UTILIZATION }}"
          export INPUT_TPS OUTPUT_TPS CACHED_TPS COST_IN COST_OUT COST_CACHED
          export PREFILL_NUM_PROMPTS=100 PREFILL_REQUEST_RATE=10 PREFILL_INPUT_LEN=3072 PREFILL_OUTPUT_LEN=1024
          export DECODE_NUM_PROMPTS=100 DECODE_REQUEST_RATE=10 DECODE_INPUT_LEN=1024 DECODE_OUTPUT_LEN=3072
          export CACHE_NUM_PROMPTS=100 CACHE_PREFIX_LEN=512 CACHE_SUFFIX_LEN=128 CACHE_NUM_PREFIXES=5 CACHE_OUTPUT_LEN=128
          
          python3 scripts/generate_results.py
          
          # Set outputs
          echo "prefill_tps=$INPUT_TPS" >> $GITHUB_OUTPUT
          echo "decode_tps=$OUTPUT_TPS" >> $GITHUB_OUTPUT
          echo "cached_tps=$CACHED_TPS" >> $GITHUB_OUTPUT
          echo "cost_in=$COST_IN" >> $GITHUB_OUTPUT
          echo "cost_out=$COST_OUT" >> $GITHUB_OUTPUT
          echo "cost_cached=$COST_CACHED" >> $GITHUB_OUTPUT

      - name: Generate final results
        run: |
          # Re-run generate_results with accuracy metrics
          export MODEL="${{ env.MODEL }}"
          export DGX_COST=$(echo "scale=4; (${{ env.DGX_HARDWARE_COST_PER_HOUR }} * ${{ env.GPU_MEMORY_UTILIZATION }}) + ${{ env.DGX_ELECTRICITY_COST_PER_HOUR }}" | bc)
          export FULL_IMAGE="${{ steps.vars.outputs.FULL_IMAGE }}"
          export GPU_MEMORY_UTILIZATION="${{ env.GPU_MEMORY_UTILIZATION }}"
          
          # Only include benchmark metrics if benchmarks were run
          if [[ "${{ github.event.inputs.accuracy_only }}" != "true" ]]; then
            export INPUT_TPS="${{ steps.benchmark.outputs.prefill_tps }}"
            export OUTPUT_TPS="${{ steps.benchmark.outputs.decode_tps }}"
            export CACHED_TPS="${{ steps.benchmark.outputs.cached_tps }}"
            export COST_IN="${{ steps.benchmark.outputs.cost_in }}"
            export COST_OUT="${{ steps.benchmark.outputs.cost_out }}"
            export COST_CACHED="${{ steps.benchmark.outputs.cost_cached }}"
            export PREFILL_NUM_PROMPTS=100 PREFILL_REQUEST_RATE=10 PREFILL_INPUT_LEN=3072 PREFILL_OUTPUT_LEN=1024
            export DECODE_NUM_PROMPTS=100 DECODE_REQUEST_RATE=10 DECODE_INPUT_LEN=1024 DECODE_OUTPUT_LEN=3072
            export CACHE_NUM_PROMPTS=100 CACHE_PREFIX_LEN=512 CACHE_SUFFIX_LEN=128 CACHE_NUM_PREFIXES=5 CACHE_OUTPUT_LEN=128
            export ACCURACY_ONLY="false"
          else
            export INPUT_TPS="0"
            export OUTPUT_TPS="0"
            export CACHED_TPS="0"
            export COST_IN="0"
            export COST_OUT="0"
            export COST_CACHED="0"
            export ACCURACY_ONLY="true"
          fi
          
          export IFEVAL_PROMPT_ACCURACY="${{ steps.ifeval.outputs.prompt_accuracy }}"
          export IFEVAL_INSTRUCTION_ACCURACY="${{ steps.ifeval.outputs.instruction_accuracy }}"
          export IFEVAL_NUM_SAMPLES="${{ steps.ifeval.outputs.num_samples }}"
          export IFEVAL_EVALUATED="true"
          
          python3 scripts/generate_results.py

      - name: Update docs
        if: success()
        run: |
          python3 scripts/update_pricing.py --results bench_results.json --html docs/index.html
          cp bench_results.json docs/benchmark-results.json
          cp ifeval_results.json docs/ifeval-results.json
          # Copy comparison results if available
          if [ -f comparison_results.json ]; then
            cp comparison_results.json docs/comparison-results.json
          fi
          
          # Update history
          python3 -c "
          import json, re, os
          with open('bench_results.json') as f: new_result = json.load(f)
          version = '${{ steps.vars.outputs.VERSION_TAG }}'
          model = '${{ env.MODEL }}'
          accuracy_only = '${{ github.event.inputs.accuracy_only }}' == 'true'
          new_result['version'] = version
          
          if not re.search(r'\b(rc|beta|alpha|dev|pre|snapshot|latest)\b', version, re.I):
              # Update benchmark history (only if not accuracy_only mode)
              if not accuracy_only:
                  try:
                      with open('docs/benchmark-history.json') as f: history = json.load(f)
                  except: history = []
                  
                  # Use version + model as composite key to allow multiple models per version
                  for i, e in enumerate(history):
                      if e.get('version') == version and e.get('model') == model:
                          history[i] = new_result
                          break
                  else:
                      history.append(new_result)
                  
                  with open('docs/benchmark-history.json', 'w') as f:
                      json.dump(history[-50:], f, indent=2)
              
              # Update accuracy history (only entries with accuracy data)
              if new_result.get('accuracy') and new_result['accuracy'].get('ifeval') and new_result['accuracy']['ifeval'].get('evaluated'):
                  try:
                      with open('docs/accuracy-history.json') as f: acc_history = json.load(f)
                  except: acc_history = []
                  
                  # Extract only accuracy-related fields
                  acc_entry = {
                      'model': new_result.get('model'),
                      'version': new_result.get('version'),
                      'timestamp': new_result.get('timestamp'),
                      'image_tag': new_result.get('image_tag'),
                      'accuracy': new_result.get('accuracy')
                  }
                  
                  # Use version + model as composite key
                  for i, e in enumerate(acc_history):
                      if e.get('version') == version and e.get('model') == model:
                          acc_history[i] = acc_entry
                          break
                  else:
                      acc_history.append(acc_entry)
                  
                  with open('docs/accuracy-history.json', 'w') as f:
                      json.dump(acc_history[-50:], f, indent=2)
          "

      - name: Commit and push
        if: success()
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add docs/*.{html,json} 2>/dev/null || true
          # Only add the specific baseline file for Llama 3.1 8B
          git add baselines/llama-3.1-8b-instruct.json 2>/dev/null || true
          git add comparison_results.json 2>/dev/null || true
          git diff --quiet --cached || {
            git commit -m "Update benchmark results [skip ci]"
            for i in {1..3}; do
              git push origin main && break || git pull --rebase origin main
            done
          }

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: |
            bench_results.json
            ifeval_results.json
            comparison_results.json

      - name: Summary
        run: |
          DGX_COST=$(echo "scale=4; (${{ env.DGX_HARDWARE_COST_PER_HOUR }} * ${{ env.GPU_MEMORY_UTILIZATION }}) + ${{ env.DGX_ELECTRICITY_COST_PER_HOUR }}" | bc)
          
          cat >> $GITHUB_STEP_SUMMARY << EOF
          ## ðŸš€ Benchmark Complete
          
          **Version:** \`${{ steps.vars.outputs.VERSION_TAG }}\`
          **Cost:** \`\$${DGX_COST}/hr\` (GPU: ${{ env.GPU_MEMORY_UTILIZATION }})
          
          ### Performance
          | Metric | Prefill | Cached | Decode |
          |--------|---------|--------|--------|
          | **Throughput (tok/s)** | ${{ steps.benchmark.outputs.prefill_tps }} | ${{ steps.benchmark.outputs.cached_tps }} | ${{ steps.benchmark.outputs.decode_tps }} |
          | **Cost per 1M tokens** | \$${{ steps.benchmark.outputs.cost_in }} | \$${{ steps.benchmark.outputs.cost_cached }} | \$${{ steps.benchmark.outputs.cost_out }} |
          
          ### Accuracy (IFEval)
          | Metric | Score |
          |--------|-------|
          | **Prompt-level accuracy** | ${{ steps.ifeval.outputs.prompt_accuracy }}% |
          | **Instruction-level accuracy** | ${{ steps.ifeval.outputs.instruction_accuracy }}% |
          | **Samples evaluated** | ${{ steps.ifeval.outputs.num_samples }} |
          
          ### Baseline Comparison
          **Status:** \`${{ steps.baseline_comparison.outputs.comparison_status || 'NOT_RUN' }}\`
          EOF
